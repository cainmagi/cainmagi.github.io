<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithm on Rosenkreutz Studio</title>
    <link>https://cainmagi.github.io/tags/algorithm/</link>
    <description>Recent content in Algorithm on Rosenkreutz Studio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Feb 2019 02:04:28 -0600</lastBuildDate>
    
	<atom:link href="https://cainmagi.github.io/tags/algorithm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Special Notes on Feb. 28, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190228special/</link>
      <pubDate>Thu, 28 Feb 2019 02:04:28 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190228special/</guid>
      <description>Introduction In this article, we would discuss the non-negative constrained least length problem. Due to the limitation of time, we only discuss the simplified form of this problem. The solution of this problem is derived from applying non-negative least squares algorithm.
KT conditions Check here to see Karush–Kuhn–Tucker conditions in Wikipedia.
 To solve this problem, first we need to introduce the Kuhn-Tucker (KT) theorem (or KT conditons).</description>
    </item>
    
    <item>
      <title>Special Notes on Feb. 27, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190227special/</link>
      <pubDate>Wed, 27 Feb 2019 14:47:48 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190227special/</guid>
      <description>Introduction In this topic, we are learning an article for studying the theory of the convergence for the neural network applied on inverse problem. To be specific, the related articles include
[1]: NETT: Solving Inverse Problems with Deep Neural Networks:
Reference
 [2]: Generalized Bregman distances and convergence rates for non-convex regularization methods:
Reference
  [3]: On Total Convexity, Bregman Projections and Stability in Banach Spaces:
Reference
 [4]: Solving ill-posed inverse problems using iterative deep neural networks:</description>
    </item>
    
    <item>
      <title>Notes on Feb. 23, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190223/</link>
      <pubDate>Sat, 23 Feb 2019 07:49:01 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190223/</guid>
      <description> Notes During this week, here is no note. A detail version about the works in this week has been updated in
Reference
Slices View the slices here:
 Ooops! Your browser does not support viewing pdfs.
Download PDF
  Ooops! Your browser does not support viewing pdfs.
Download PDF
 </description>
    </item>
    
    <item>
      <title>Stochastic optimization I: from Monte-Carlo methods to Gibbs sampling</title>
      <link>https://cainmagi.github.io/notes/note20190215special/</link>
      <pubDate>Fri, 15 Feb 2019 13:04:50 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190215special/</guid>
      <description>Introduction This is a series of inspection on stochastic methods. Traditionally, an optimization problem could be solved by gradient descent methods, greedy algorithm and stochastic methods. For example, Levenberg–Marquardt algorithm is a gradient descent based methods. Another example is OMP which is used to find a local minimum of L0 penalized problem. Since the L0 norm is totally indifferentiable, we could not calculate its gradient. Therefore, such kind of problem is generally more difficult than those differentiable problems.</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 24, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180824special/</link>
      <pubDate>Fri, 24 Aug 2018 03:45:41 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180824special/</guid>
      <description>Introduction Check here to see Levenberg–Marquardt algorithm in Wikipedia.
 Check this link and we could review the theory of Levenberg–Marquardt algorithm (LMA), which is used as an improvement compared to the plain first-order gradient descent method. In this short discussion, we would like to talk about how and why we could derive such a method. Then we would know that when we could use this method and when we could not.</description>
    </item>
    
  </channel>
</rss>