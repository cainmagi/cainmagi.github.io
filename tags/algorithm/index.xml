<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithm on Rosenkreutz Studio</title>
    <link>https://cainmagi.github.io/tags/algorithm/</link>
    <description>Recent content in Algorithm on Rosenkreutz Studio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Feb 2019 13:04:50 -0600</lastBuildDate>
    
	<atom:link href="https://cainmagi.github.io/tags/algorithm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Stochastic optimization I: from Monte-Carlo methods to Gibbs sampling</title>
      <link>https://cainmagi.github.io/notes/note20190215sp/</link>
      <pubDate>Fri, 15 Feb 2019 13:04:50 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190215sp/</guid>
      <description>
This article is still being produced. Please wait for several days to see the full edition.
 </description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 24, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180824special/</link>
      <pubDate>Fri, 24 Aug 2018 03:45:41 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180824special/</guid>
      <description>Introduction Check here to see Levenberg–Marquardt algorithm in Wikipedia.
 Check this link and we could review the theory of Levenberg–Marquardt algorithm (LMA), which is used as an improvement compared to the plain first-order gradient descent method. In this short discussion, we would like to talk about how and why we could derive such a method. Then we would know that when we could use this method and when we could not.</description>
    </item>
    
  </channel>
</rss>