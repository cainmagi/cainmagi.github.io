<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Optimization on Rosenkreutz Studio</title>
    <link>https://cainmagi.github.io/tags/optimization/</link>
    <description>Recent content in Optimization on Rosenkreutz Studio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Feb 2019 13:04:50 -0600</lastBuildDate>
    
	<atom:link href="https://cainmagi.github.io/tags/optimization/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Stochastic optimization I: from Monte-Carlo methods to Gibbs sampling</title>
      <link>https://cainmagi.github.io/notes/note20190215sp/</link>
      <pubDate>Fri, 15 Feb 2019 13:04:50 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190215sp/</guid>
      <description>
This article is still being produced. Please wait for several days to see the full edition.
 </description>
    </item>
    
    <item>
      <title>Special Notes on Nov. 19, 2018</title>
      <link>https://cainmagi.github.io/notes/note20181129special/</link>
      <pubDate>Mon, 19 Nov 2018 17:25:43 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20181129special/</guid>
      <description>Theory of Lovasz extension The Lovasz extension is proposed to form an interpolating function for a sub-modular set function. In this article, we are discussing about the theory and the results from this paper: &amp;ldquo;The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks&amp;rdquo;,
Reference
To get more details, we have the further reading material about Lovasz-hinge optimization: &amp;ldquo;The Lovász Hinge: A Novel Convex Surrogate for Submodular Losses&amp;ldquo;</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 24, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180824special/</link>
      <pubDate>Fri, 24 Aug 2018 03:45:41 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180824special/</guid>
      <description>Introduction Check here to see Levenberg–Marquardt algorithm in Wikipedia.
 Check this link and we could review the theory of Levenberg–Marquardt algorithm (LMA), which is used as an improvement compared to the plain first-order gradient descent method. In this short discussion, we would like to talk about how and why we could derive such a method. Then we would know that when we could use this method and when we could not.</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 13, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180813special/</link>
      <pubDate>Mon, 13 Aug 2018 01:45:25 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180813special/</guid>
      <description>Introduction In this note, we would like to discuss about an interesting idea: how to implement the conventional optimization methods in deep-learning architecture? Actually we have introduced this idea in note20180720. Here let us give a brief about this idea.
A brief about inversion First, we need to learn how the traditional inversion works. This process could be generally described as such a process:
   The workflow of a traditional inversion         Suppose we have a known data $\mathbf{y}_0$, a forward model function $\mathcal{F}$ could transform a model $\mathbf{x}$ into data $\mathbf{y}$.</description>
    </item>
    
    <item>
      <title>Special Notes on Jun. 05, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180605special/</link>
      <pubDate>Tue, 05 Jun 2018 00:49:31 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180605special/</guid>
      <description>Introduction The baseline of this work is existing asynchronous SGD algorithms including Hogwild, SVRG and SAGA. The author summarize the work as
 General composite optimization problem:
\begin{equation} \begin{aligned} \arg \min\limits_{\mathbf{x} \in \mathbb{R}^p} &amp; f(\mathbf{x}) + h(\mathbf{x}),\\[5pt] \mathrm{s.t.}~&amp;f(\mathbf{x}) := \frac{1}{n} \sum_{i=1}^n f_i(\mathbf{x}), \end{aligned} \end{equation}  where each $f_i$ is convex with L-Lipschitz gradient (i.e. L-smooth), and the averaging function is $\mu$-strongly convex. $h$ is convex but potentially non-smooth, and it could be decomposed block coordinate-wise as</description>
    </item>
    
  </channel>
</rss>