<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inverse Problem on Rosenkreutz Studio</title>
    <link>https://cainmagi.github.io/tags/inverse-problem/</link>
    <description>Recent content in Inverse Problem on Rosenkreutz Studio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Feb 2019 02:04:28 -0600</lastBuildDate>
    
	<atom:link href="https://cainmagi.github.io/tags/inverse-problem/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Special Notes on Feb. 28, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190228special/</link>
      <pubDate>Thu, 28 Feb 2019 02:04:28 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190228special/</guid>
      <description>Introduction In this article, we would discuss the non-negative constrained least length problem. Due to the limitation of time, we only discuss the simplified form of this problem. The solution of this problem is derived from applying non-negative least squares algorithm.
KT conditions Check here to see Karush–Kuhn–Tucker conditions in Wikipedia.
 To solve this problem, first we need to introduce the Kuhn-Tucker (KT) theorem (or KT conditons).</description>
    </item>
    
    <item>
      <title>Special Notes on Feb. 27, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190227special/</link>
      <pubDate>Wed, 27 Feb 2019 14:47:48 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190227special/</guid>
      <description>Introduction In this topic, we are learning an article for studying the theory of the convergence for the neural network applied on inverse problem. To be specific, the related articles include
[1]: NETT: Solving Inverse Problems with Deep Neural Networks:
Reference
 [2]: Generalized Bregman distances and convergence rates for non-convex regularization methods:
Reference
  [3]: On Total Convexity, Bregman Projections and Stability in Banach Spaces:
Reference
 [4]: Solving ill-posed inverse problems using iterative deep neural networks:</description>
    </item>
    
    <item>
      <title>Guide book for Tensorflow</title>
      <link>https://cainmagi.github.io/projects/web_tensorflowguide/</link>
      <pubDate>Mon, 25 Feb 2019 16:18:38 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/projects/web_tensorflowguide/</guid>
      <description>Redirection 欢迎来到本页面！如果你的页面没有自动跳转，很有可能是因为你的浏览器禁止了这一功能，那么，请点击以下链接查看详情：
Tensorflow手札</description>
    </item>
    
    <item>
      <title>Notes from Jan. 29, 2019 to Feb. 15, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190129/</link>
      <pubDate>Tue, 29 Jan 2019 17:23:42 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190129/</guid>
      <description>Introduction In this article we would summarize some popular solutions for the inverse problem. To be specific, here we only discuss the methods of inverse problems. Although when introducing some algorithms, we may need to explain what problem they work on, the various topics about which problem we solve is not what we concentrate on in this article.
Generally the solutions could be divided into 2 parts: optimizing methods and stochastic approaches.</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 24, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180824special/</link>
      <pubDate>Fri, 24 Aug 2018 03:45:41 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180824special/</guid>
      <description>Introduction Check here to see Levenberg–Marquardt algorithm in Wikipedia.
 Check this link and we could review the theory of Levenberg–Marquardt algorithm (LMA), which is used as an improvement compared to the plain first-order gradient descent method. In this short discussion, we would like to talk about how and why we could derive such a method. Then we would know that when we could use this method and when we could not.</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 13, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180813special/</link>
      <pubDate>Mon, 13 Aug 2018 01:45:25 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180813special/</guid>
      <description>Introduction In this note, we would like to discuss about an interesting idea: how to implement the conventional optimization methods in deep-learning architecture? Actually we have introduced this idea in note20180720. Here let us give a brief about this idea.
A brief about inversion First, we need to learn how the traditional inversion works. This process could be generally described as such a process:
   The workflow of a traditional inversion         Suppose we have a known data $\mathbf{y}_0$, a forward model function $\mathcal{F}$ could transform a model $\mathbf{x}$ into data $\mathbf{y}$.</description>
    </item>
    
    <item>
      <title>Notes on Jul. 20, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180720/</link>
      <pubDate>Fri, 20 Jul 2018 05:37:52 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180720/</guid>
      <description>Introduction Here we would like to discuss about the some papers using deep learning methods to enhance the performance of the traditional inverse problems. The application should be limited in analyzing the electromagnetic wave or acoustic signal. Some works just make the application of existing methods, some works propose fundamental theory and some works give us an inspiration of available architectures. According to the relevance, we would like to divide them into 3 groups.</description>
    </item>
    
    <item>
      <title>Tensorflow Inspection for FWM Curves 180602</title>
      <link>https://cainmagi.github.io/projects/python_tensorfwm201806/</link>
      <pubDate>Fri, 06 Jul 2018 13:52:21 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_tensorfwm201806/</guid>
      <description>Introduction In this project, we combine a deep-learning architecture with a traditional electromagnetic (EM) forward model which is differentiable. Since the model is differentiable, we could enable the gradient back propagated from the model to the deep-learning network. The following figure could be used to describe this design.
   The whole architecture of the project         Suppose we have gotten a group of response samples (i.</description>
    </item>
    
    <item>
      <title>Forward Model : Curves 180602</title>
      <link>https://cainmagi.github.io/projects/python_fwm201806/</link>
      <pubDate>Mon, 18 Jun 2018 12:55:34 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_fwm201806/</guid>
      <description>C++ Migrated Project Introduction This is a python-c-api that wrapping the C++ forward model codes (which is supported by OpenMP) with Numpy-API. To get the C++ codes, visit the master branch:
Master
 Note that the project is still private now, thus you may do not have the authority to visit this page.  The function prototype could be described as
float64_t *resp = curves(numLayers, Rh, Rv, Zbed, Dip, TVD);  This function is used to simulate the response of the azimuthal resistivity LWD tool.</description>
    </item>
    
  </channel>
</rss>