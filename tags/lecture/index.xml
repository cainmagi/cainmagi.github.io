<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>lecture on Rosenkreutz Studio</title>
    <link>https://cainmagi.github.io/tags/lecture/</link>
    <description>Recent content in lecture on Rosenkreutz Studio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Aug 2019 02:02:02 -0500</lastBuildDate>
    
	<atom:link href="https://cainmagi.github.io/tags/lecture/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Chinese translated lecture notes for MATH 6366 @ UH</title>
      <link>https://cainmagi.github.io/notes/note20190820special/</link>
      <pubDate>Tue, 20 Aug 2019 02:02:02 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190820special/</guid>
      <description> 
这个项目还在施工中！如果你发现本译丛有任何问题，欢迎在下方的讨论版指正，译者不胜感激。
 </description>
    </item>
    
    <item>
      <title>Stochastic optimization I: from Monte-Carlo methods to Gibbs sampling</title>
      <link>https://cainmagi.github.io/notes/note20190215special/</link>
      <pubDate>Fri, 15 Feb 2019 13:04:50 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190215special/</guid>
      <description>Introduction This is a series of inspection on stochastic methods. Traditionally, an optimization problem could be solved by gradient descent methods, greedy algorithm and stochastic methods. For example, Levenberg–Marquardt algorithm is a gradient descent based methods. Another example is OMP which is used to find a local minimum of L0 penalized problem. Since the L0 norm is totally indifferentiable, we could not calculate its gradient. Therefore, such kind of problem is generally more difficult than those differentiable problems.</description>
    </item>
    
  </channel>
</rss>