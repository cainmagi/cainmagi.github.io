<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Rosenkreutz Studio</title>
    <link>https://cainmagi.github.io/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on Rosenkreutz Studio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Feb 2019 16:18:38 -0600</lastBuildDate>
    
	<atom:link href="https://cainmagi.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Guide book for Tensorflow</title>
      <link>https://cainmagi.github.io/projects/web_tensorflowguide/</link>
      <pubDate>Mon, 25 Feb 2019 16:18:38 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/projects/web_tensorflowguide/</guid>
      <description>Redirection 欢迎来到本页面！如果你的页面没有自动跳转，很有可能是因为你的浏览器禁止了这一功能，那么，请点击以下链接查看详情：
Tensorflow手札</description>
    </item>
    
    <item>
      <title>Notes from Jan. 29, 2019 to Feb. 15, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190129/</link>
      <pubDate>Tue, 29 Jan 2019 17:23:42 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190129/</guid>
      <description>Introduction In this article we would summarize some popular solutions for the inverse problem. To be specific, here we only discuss the methods of inverse problems. Although when introducing some algorithms, we may need to explain what problem they work on, the various topics about which problem we solve is not what we concentrate on in this article.
Generally the solutions could be divided into 2 parts: optimizing methods and stochastic approaches.</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 24, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180824special/</link>
      <pubDate>Fri, 24 Aug 2018 03:45:41 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180824special/</guid>
      <description>Introduction Check here to see Levenberg–Marquardt algorithm in Wikipedia.
 Check this link and we could review the theory of Levenberg–Marquardt algorithm (LMA), which is used as an improvement compared to the plain first-order gradient descent method. In this short discussion, we would like to talk about how and why we could derive such a method. Then we would know that when we could use this method and when we could not.</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 13, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180813special/</link>
      <pubDate>Mon, 13 Aug 2018 01:45:25 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180813special/</guid>
      <description>Introduction In this note, we would like to discuss about an interesting idea: how to implement the conventional optimization methods in deep-learning architecture? Actually we have introduced this idea in note20180720. Here let us give a brief about this idea.
A brief about inversion First, we need to learn how the traditional inversion works. This process could be generally described as such a process:
   The workflow of a traditional inversion         Suppose we have a known data $\mathbf{y}_0$, a forward model function $\mathcal{F}$ could transform a model $\mathbf{x}$ into data $\mathbf{y}$.</description>
    </item>
    
    <item>
      <title>Notes on Jul. 20, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180720/</link>
      <pubDate>Fri, 20 Jul 2018 05:37:52 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180720/</guid>
      <description>Introduction Here we would like to discuss about the some papers using deep learning methods to enhance the performance of the traditional inverse problems. The application should be limited in analyzing the electromagnetic wave or acoustic signal. Some works just make the application of existing methods, some works propose fundamental theory and some works give us an inspiration of available architectures. According to the relevance, we would like to divide them into 3 groups.</description>
    </item>
    
    <item>
      <title>Tensorflow Inspection for FWM Curves 180602</title>
      <link>https://cainmagi.github.io/projects/python_tensorfwm201806/</link>
      <pubDate>Fri, 06 Jul 2018 13:52:21 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_tensorfwm201806/</guid>
      <description>Introduction In this project, we combine a deep-learning architecture with a traditional electromagnetic (EM) forward model which is differentiable. Since the model is differentiable, we could enable the gradient back propagated from the model to the deep-learning network. The following figure could be used to describe this design.
   The whole architecture of the project         Suppose we have gotten a group of response samples (i.</description>
    </item>
    
    <item>
      <title>Special Notes on Jun. 05, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180605special/</link>
      <pubDate>Tue, 05 Jun 2018 00:49:31 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180605special/</guid>
      <description>Introduction The baseline of this work is existing asynchronous SGD algorithms including Hogwild, SVRG and SAGA. The author summarize the work as
 General composite optimization problem:
\begin{equation} \begin{aligned} \arg \min\limits_{\mathbf{x} \in \mathbb{R}^p} &amp; f(\mathbf{x}) + h(\mathbf{x}),\\[5pt] \mathrm{s.t.}~&amp;f(\mathbf{x}) := \frac{1}{n} \sum_{i=1}^n f_i(\mathbf{x}), \end{aligned} \end{equation}  where each $f_i$ is convex with L-Lipschitz gradient (i.e. L-smooth), and the averaging function is $\mu$-strongly convex. $h$ is convex but potentially non-smooth, and it could be decomposed block coordinate-wise as</description>
    </item>
    
    <item>
      <title>Notes on May 26, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180526/</link>
      <pubDate>Sat, 26 May 2018 02:10:27 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180526/</guid>
      <description>Set-invariant network Check here to see the article Deep Sets:
Reference
Theory This article discusses about how to build a neural network with order-invariant input set.
 Theorem 1: which function could be set invariant:  Such a function $f(\mathbb{X})$ need to be able to be decomposed as such a form: \begin{align} f(\mathbb{X}) = \rho(\sum\limits_{x \in \mathbb{X}} \phi(x)). \end{align} 
 Since the general form of the layer of neural network is $F(\mathbf{x},~\boldsymbol{\Theta}) = \sigma (\boldsymbol{\Theta}\mathbf{x})$, we could know that according to the above theorem, only when</description>
    </item>
    
    <item>
      <title>Notes on May 18, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180518/</link>
      <pubDate>Fri, 25 May 2018 17:32:01 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180518/</guid>
      <description> Notes During this week (May 18, 2018) I have not build this site. So here is no note.
Slices View the slices here:
 Ooops! Your browser does not support viewing pdfs.
Download PDF
 </description>
    </item>
    
    <item>
      <title>Deep Learning for AGT</title>
      <link>https://cainmagi.github.io/projects/python_agt201804/</link>
      <pubDate>Fri, 25 May 2018 09:42:19 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_agt201804/</guid>
      <description>Introduction This project is for AGT (Advanced Geophysical Technology) and 2018 SEG (Society of Exploration Geophysicists) conference. We concentrate on the seismic raw data and use deep learning approaches to improve the performance in some special application.
Beat Tone Network The first application is for improving the FWI of Beat Tone, see here to learn the background of this application:
Reference
When the basic signal frequency increases, the effectiveness of the Beat Tone method reduces significantly.</description>
    </item>
    
    <item>
      <title>Real-time Shaker Video Analysis for Shell Project</title>
      <link>https://cainmagi.github.io/projects/python_shell201712/</link>
      <pubDate>Fri, 25 May 2018 09:42:05 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_shell201712/</guid>
      <description>Introduction In this project, we aim to process video stream by utilizing deep learning tools (tensorflow). The data is from the camera deployed beside the real shaker on well-logging site. When the shaker is spare, the surface of it is clean. However, during the well-logging process, the cuttings flow would come and get captured by the camera.
To build a complete system, we divde our work into two phases.</description>
    </item>
    
    <item>
      <title>Deep Learning Utilities</title>
      <link>https://cainmagi.github.io/projects/python_deeputilities/</link>
      <pubDate>Fri, 25 May 2018 09:41:43 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_deeputilities/</guid>
      <description>Introduction Processing data with tensorflow (python) could be accelerated with utilizing advanced CPU drivers and GPU. In this case, the pre-processing and post-processing may delay the fast GPU processing. For example, one of the most ordinary application is converting the segmentation mask picture to the multi-channel mask.
Matrix projecting As is shown below, in the segmentation picture, we use each color to represent a label. But in real application, to avoid the overlapping of the label value, we need to separate each label in one-hot format, i.</description>
    </item>
    
  </channel>
</rss>