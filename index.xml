<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rosenkreutz Studio</title>
    <link>https://cainmagi.github.io/</link>
    <description>Recent content on Rosenkreutz Studio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Feb 2019 02:04:28 -0600</lastBuildDate>
    
	<atom:link href="https://cainmagi.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Special Notes on Feb. 28, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190228special/</link>
      <pubDate>Thu, 28 Feb 2019 02:04:28 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190228special/</guid>
      <description>Introduction In this article, we would discuss the non-negative constrained least length problem. Due to the limitation of time, we only discuss the simplified form of this problem. The solution of this problem is derived from applying non-negative least squares algorithm.
KT conditions Check here to see Karush–Kuhn–Tucker conditions in Wikipedia.
 To solve this problem, first we need to introduce the Kuhn-Tucker (KT) theorem (or KT conditons).</description>
    </item>
    
    <item>
      <title>Special Notes on Feb. 27, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190227special/</link>
      <pubDate>Wed, 27 Feb 2019 14:47:48 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190227special/</guid>
      <description>Introduction In this topic, we are learning an article for studying the theory of the convergence for the neural network applied on inverse problem. To be specific, the related articles include
[1]: NETT: Solving Inverse Problems with Deep Neural Networks:
Reference
 [2]: Generalized Bregman distances and convergence rates for non-convex regularization methods:
Reference
  [3]: On Total Convexity, Bregman Projections and Stability in Banach Spaces:
Reference
 [4]: Solving ill-posed inverse problems using iterative deep neural networks:</description>
    </item>
    
    <item>
      <title>Guide book for Tensorflow</title>
      <link>https://cainmagi.github.io/projects/web_tensorflowguide/</link>
      <pubDate>Mon, 25 Feb 2019 16:18:38 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/projects/web_tensorflowguide/</guid>
      <description>Redirection 欢迎来到本页面！如果你的页面没有自动跳转，很有可能是因为你的浏览器禁止了这一功能，那么，请点击以下链接查看详情：
Tensorflow手札</description>
    </item>
    
    <item>
      <title>Customize MkDocs-Material with Javascript</title>
      <link>https://cainmagi.github.io/playground/20190225mkdocs/</link>
      <pubDate>Mon, 25 Feb 2019 00:17:46 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/playground/20190225mkdocs/</guid>
      <description>背景 这是一篇中文教程，其目的主要是为最先进的MkDocs主题之一，Material提供一个更丰富的中文支持。部分操作需要修改模板内建的文件。截至本文写就的时候，笔者正在使用Python 3.6，MkDocs 1.0.4，以及Material 4.0.1。本文基于的模板Material，是MkDocs模板里集成了最多实用插件的模板之一，涵盖了绝大部分来自MarkDown extensions，和PyMdown Extensions的插件。已经包含了诸多妙用。该模板美观、现代，清晰易读，并且同时支持桌面和手机版，即使不用来写文档，也是一个非常优秀的博客站，欲了解更多关于Material的信息，参考以下链接：
Material
然而，该模板本身对中文的支持十分有限。鉴于此，本教程参考了以下的一些资料：
mkdocs如何支持中文搜索
为 lunr.js 添加中文支持
事实上，Material已经支持了MathJax、代码高亮、Tip框、保留代码增删的注释等功能。不过在本教程里，基本不会讨论一些站点参数的设置、以及无关本文的插件的用法，读者最好应当有过使用Hugo，Hexo或者PyMarkDown等类似工具的经验。
安装 首先，对于没有安装MkDocs的用户而言，需要
pip install mkdocs  如果你的Python在你的环境变量里，或者你在使用Anaconda，那么就可以直接调用mkdocs --version来检查安装情况了。接下来需要安装一些插件
pip install Pygments pymdown-extensions  由于我们需要修改模板本身，为了灵活方便起见，这里不建议使用安装的方式（例如pip）将Material安装到库里，反之，我们则应当直接clone项目。对于安装了Github Desktop的Windows用户，可以通过
github clone https://github.com/squidfunk/mkdocs-material.git  将该项目的最新版（目前是4.0.1）Fork到你的常用项目目录里（喜欢使用git的也无妨）。接下来，在根目录下，删除所有无用的文件，最后只需要保留到
. |---.github |---docs |---material |---CODE_OF_CONDUCT.md |---CONTRIBUTING.md |---LICENSE |---mkdocs.yml `---README.md  事实上，根目录下除了mkdocs.yml属于模板的基本配置文件以外，其他的都是和模板本身无关的文件。例如./src目录保存的基本都是还没有压缩处理过的模板源文件。这些源文件亲测不能用来代替压缩后的模板./material。在Linux下，用户可以通过Makefile完成从./src编译到./material的过程，但是Windows下这一操作并不方便，所以在下文中，尽管有些修改原理是基于源文件介绍的，但实际情况下我们需要直接修改被压缩后的，可读性大大变差的模板文件，不得不说是一个费时不讨好的工作。
改进 预备 新建3个空文件main.css，extensions.css和extensions.js，方便后续的插件配置。
. |---.github |---docs | |---stylesheets | | |----main.css | | |----extensions.css | | `---... | `---javascripts | |----extensions.js | `---... |---material `---.</description>
    </item>
    
    <item>
      <title>Notes on Feb. 23, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190223/</link>
      <pubDate>Sat, 23 Feb 2019 07:49:01 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190223/</guid>
      <description> Notes During this week, here is no note. A detail version about the works in this week has been updated in
Reference
Slices View the slices here:
 Ooops! Your browser does not support viewing pdfs.
Download PDF
  Ooops! Your browser does not support viewing pdfs.
Download PDF
 </description>
    </item>
    
    <item>
      <title>Stochastic optimization I: from Monte-Carlo methods to Gibbs sampling</title>
      <link>https://cainmagi.github.io/notes/note20190215special/</link>
      <pubDate>Fri, 15 Feb 2019 13:04:50 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190215special/</guid>
      <description>Introduction This is a series of inspection on stochastic methods. Traditionally, an optimization problem could be solved by gradient descent methods, greedy algorithm and stochastic methods. For example, Levenberg–Marquardt algorithm is a gradient descent based methods. Another example is OMP which is used to find a local minimum of L0 penalized problem. Since the L0 norm is totally indifferentiable, we could not calculate its gradient. Therefore, such kind of problem is generally more difficult than those differentiable problems.</description>
    </item>
    
    <item>
      <title>Latex Templates</title>
      <link>https://cainmagi.github.io/projects/latex_templates/</link>
      <pubDate>Tue, 05 Feb 2019 17:26:37 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/projects/latex_templates/</guid>
      <description>Introduction This project contains all $\LaTeX$ templates designed by myself. Now it only contains some Beamer templates. In the future, I would upload more templates if necessary.
All of these templates are tested on $\TeX\mathrm{Live}$ and Windows. Some reports show that these templates may not compatible with MikTex, MacTex and some other platforms. If you want to use these templates in those environment, you may have to solve the incompatible problems by yourself.</description>
    </item>
    
    <item>
      <title>Notes from Jan. 29, 2019 to Feb. 15, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190129/</link>
      <pubDate>Tue, 29 Jan 2019 17:23:42 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190129/</guid>
      <description>Introduction In this article we would summarize some popular solutions for the inverse problem. To be specific, here we only discuss the methods of inverse problems. Although when introducing some algorithms, we may need to explain what problem they work on, the various topics about which problem we solve is not what we concentrate on in this article.
Generally the solutions could be divided into 2 parts: optimizing methods and stochastic approaches.</description>
    </item>
    
    <item>
      <title>Special Notes on Nov. 19, 2018</title>
      <link>https://cainmagi.github.io/notes/note20181129special/</link>
      <pubDate>Mon, 19 Nov 2018 17:25:43 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20181129special/</guid>
      <description>Theory of Lovasz extension The Lovasz extension is proposed to form an interpolating function for a sub-modular set function. In this article, we are discussing about the theory and the results from this paper: &amp;ldquo;The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks&amp;rdquo;,
Reference
To get more details, we have the further reading material about Lovasz-hinge optimization: &amp;ldquo;The Lovász Hinge: A Novel Convex Surrogate for Submodular Losses&amp;ldquo;</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 24, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180824special/</link>
      <pubDate>Fri, 24 Aug 2018 03:45:41 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180824special/</guid>
      <description>Introduction Check here to see Levenberg–Marquardt algorithm in Wikipedia.
 Check this link and we could review the theory of Levenberg–Marquardt algorithm (LMA), which is used as an improvement compared to the plain first-order gradient descent method. In this short discussion, we would like to talk about how and why we could derive such a method. Then we would know that when we could use this method and when we could not.</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 13, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180813special/</link>
      <pubDate>Mon, 13 Aug 2018 01:45:25 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180813special/</guid>
      <description>Introduction In this note, we would like to discuss about an interesting idea: how to implement the conventional optimization methods in deep-learning architecture? Actually we have introduced this idea in note20180720. Here let us give a brief about this idea.
A brief about inversion First, we need to learn how the traditional inversion works. This process could be generally described as such a process:
   The workflow of a traditional inversion         Suppose we have a known data $\mathbf{y}_0$, a forward model function $\mathcal{F}$ could transform a model $\mathbf{x}$ into data $\mathbf{y}$.</description>
    </item>
    
    <item>
      <title>Report for peroidically hibernation of Nvidia DGX 230 on Aug. 11, 2018</title>
      <link>https://cainmagi.github.io/playground/20180811nvidiabugs/</link>
      <pubDate>Sat, 11 Aug 2018 17:29:51 +0000</pubDate>
      
      <guid>https://cainmagi.github.io/playground/20180811nvidiabugs/</guid>
      <description>Problem statement  What is it?: We assume that this problem is caused by the hibernation of the graphics cards. How does it happen?: This problem occurs periodically. Every week it would occur for about once. It is not due to some special applications, because even when there is no burden on GPU, it still happens. What does it cause?: Here are some phenomenons when the problem appears:  Cannot turn on the screen: The screen of DGX would be kept off.</description>
    </item>
    
    <item>
      <title>Notes on Jul. 20, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180720/</link>
      <pubDate>Fri, 20 Jul 2018 05:37:52 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180720/</guid>
      <description>Introduction Here we would like to discuss about the some papers using deep learning methods to enhance the performance of the traditional inverse problems. The application should be limited in analyzing the electromagnetic wave or acoustic signal. Some works just make the application of existing methods, some works propose fundamental theory and some works give us an inspiration of available architectures. According to the relevance, we would like to divide them into 3 groups.</description>
    </item>
    
    <item>
      <title>Tensorflow Inspection for FWM Curves 180602</title>
      <link>https://cainmagi.github.io/projects/python_tensorfwm201806/</link>
      <pubDate>Fri, 06 Jul 2018 13:52:21 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_tensorfwm201806/</guid>
      <description>Introduction In this project, we combine a deep-learning architecture with a traditional electromagnetic (EM) forward model which is differentiable. Since the model is differentiable, we could enable the gradient back propagated from the model to the deep-learning network. The following figure could be used to describe this design.
   The whole architecture of the project         Suppose we have gotten a group of response samples (i.</description>
    </item>
    
    <item>
      <title>Forward Model : Curves 180602</title>
      <link>https://cainmagi.github.io/projects/python_fwm201806/</link>
      <pubDate>Mon, 18 Jun 2018 12:55:34 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_fwm201806/</guid>
      <description>C++ Migrated Project Introduction This is a python-c-api that wrapping the C++ forward model codes (which is supported by OpenMP) with Numpy-API. To get the C++ codes, visit the master branch:
Master
 Note that the project is still private now, thus you may do not have the authority to visit this page.  The function prototype could be described as
float64_t *resp = curves(numLayers, Rh, Rv, Zbed, Dip, TVD);  This function is used to simulate the response of the azimuthal resistivity LWD tool.</description>
    </item>
    
    <item>
      <title>Advanced Linux Skills for Using NVIDIA Docker: Questions and Answers</title>
      <link>https://cainmagi.github.io/playground/20180614linuxdockerqna/</link>
      <pubDate>Thu, 14 Jun 2018 19:37:05 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/playground/20180614linuxdockerqna/</guid>
      <description>Instruction Here we collect some common errors caused by mistaking operations. Note that because of the limitation of the docker environment, some errors are unsolvable. When you meet such errors, it is better to delete the broken image and retrieve the backup version.
In the following parts, the questions would be categorized by different classes. Please use CTRL+F or search for your problem by checking these classes. If you meet a problem that is not discussed here, it is welcome for you to propose it in the discussion board at the bottom of this page.</description>
    </item>
    
    <item>
      <title>Advanced Linux Skills for Using NVIDIA Docker II: with VNC</title>
      <link>https://cainmagi.github.io/playground/20180614linuxdocker/</link>
      <pubDate>Thu, 14 Jun 2018 19:36:49 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/playground/20180614linuxdocker/</guid>
      <description>Instruction Before reading this article, please make sure that you have understood how to use docker (or read the prior article). Here is the prior instruction that gives you a whole imagination about docker. Some techniques (like how to save/read images or how to log in NVIDIA cloud) would not be introduced here particularly. We strongly recommend you to read this article after understanding the prior one.
Prior Instructions</description>
    </item>
    
    <item>
      <title>Special Notes on Jun. 05, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180605special/</link>
      <pubDate>Tue, 05 Jun 2018 00:49:31 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180605special/</guid>
      <description>Introduction The baseline of this work is existing asynchronous SGD algorithms including Hogwild, SVRG and SAGA. The author summarize the work as
 General composite optimization problem:
\begin{equation} \begin{aligned} \arg \min\limits_{\mathbf{x} \in \mathbb{R}^p} &amp; f(\mathbf{x}) + h(\mathbf{x}),\\[5pt] \mathrm{s.t.}~&amp;f(\mathbf{x}) := \frac{1}{n} \sum_{i=1}^n f_i(\mathbf{x}), \end{aligned} \end{equation}  where each $f_i$ is convex with L-Lipschitz gradient (i.e. L-smooth), and the averaging function is $\mu$-strongly convex. $h$ is convex but potentially non-smooth, and it could be decomposed block coordinate-wise as</description>
    </item>
    
    <item>
      <title>Advanced Linux Skills for Using NVIDIA Docker</title>
      <link>https://cainmagi.github.io/playground/20180531linuxdocker/</link>
      <pubDate>Thu, 31 May 2018 23:43:48 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/playground/20180531linuxdocker/</guid>
      <description>Instruction   This is an instruction for those who want to know and make use of NVIDIA docker. In this section we would introduce what is docker and why we choose docker. If you have already know these, you could skip this section and get to the next one.
What is docker Before we talk about docker, to be specific, we need to introduce you some concepts:
   Name Meaning     Docker A tool for managing containers.</description>
    </item>
    
    <item>
      <title>Basic Linux Skills for Remote Controlling</title>
      <link>https://cainmagi.github.io/playground/20180526linuxskill/</link>
      <pubDate>Sat, 26 May 2018 17:01:12 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/playground/20180526linuxskill/</guid>
      <description>Instruction Here we provides some basic instructions for how to access to the remote Linux server. We assume that the reader who learn this instruction could grant such conditions:
 Use a client device equipped with Windows; Has the full authority on both the client and the server; The client and the server is in the same domain.  To perform a test of whether it is ok for network equipments, type this command in Windows CMD:</description>
    </item>
    
    <item>
      <title>Notes on May 26, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180526/</link>
      <pubDate>Sat, 26 May 2018 02:10:27 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180526/</guid>
      <description>Set-invariant network Check here to see the article Deep Sets:
Reference
Theory This article discusses about how to build a neural network with order-invariant input set.
 Theorem 1: which function could be set invariant:  Such a function $f(\mathbb{X})$ need to be able to be decomposed as such a form: \begin{align} f(\mathbb{X}) = \rho(\sum\limits_{x \in \mathbb{X}} \phi(x)). \end{align} 
 Since the general form of the layer of neural network is $F(\mathbf{x},~\boldsymbol{\Theta}) = \sigma (\boldsymbol{\Theta}\mathbf{x})$, we could know that according to the above theorem, only when</description>
    </item>
    
    <item>
      <title>Notes on May 18, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180518/</link>
      <pubDate>Fri, 25 May 2018 17:32:01 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180518/</guid>
      <description> Notes During this week (May 18, 2018) I have not build this site. So here is no note.
Slices View the slices here:
 Ooops! Your browser does not support viewing pdfs.
Download PDF
 </description>
    </item>
    
    <item>
      <title>Bibile Study Notes of UH Group</title>
      <link>https://cainmagi.github.io/playground/20180525biblestudy/</link>
      <pubDate>Fri, 25 May 2018 09:43:00 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/playground/20180525biblestudy/</guid>
      <description>Introduction This is a latex project of the Thursday fellowship in UH religion center. Although I attempt to build a full collection of our bible study progress, I have failed on this work. Now it only contains notes for several but not all meetings. Some of these notes are materials when I need to host the meeting.
Note that the language is Chinese.
Contents of the Book Now it contents:</description>
    </item>
    
    <item>
      <title>Deep Learning for AGT</title>
      <link>https://cainmagi.github.io/projects/python_agt201804/</link>
      <pubDate>Fri, 25 May 2018 09:42:19 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_agt201804/</guid>
      <description>Introduction This project is for AGT (Advanced Geophysical Technology) and 2018 SEG (Society of Exploration Geophysicists) conference. We concentrate on the seismic raw data and use deep learning approaches to improve the performance in some special application.
Beat Tone Network The first application is for improving the FWI of Beat Tone, see here to learn the background of this application:
Reference
When the basic signal frequency increases, the effectiveness of the Beat Tone method reduces significantly.</description>
    </item>
    
    <item>
      <title>Real-time Shaker Video Analysis for Shell Project</title>
      <link>https://cainmagi.github.io/projects/python_shell201712/</link>
      <pubDate>Fri, 25 May 2018 09:42:05 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_shell201712/</guid>
      <description>Introduction In this project, we aim to process video stream by utilizing deep learning tools (tensorflow). The data is from the camera deployed beside the real shaker on well-logging site. When the shaker is spare, the surface of it is clean. However, during the well-logging process, the cuttings flow would come and get captured by the camera.
To build a complete system, we divde our work into two phases.</description>
    </item>
    
    <item>
      <title>Deep Learning Utilities</title>
      <link>https://cainmagi.github.io/projects/python_deeputilities/</link>
      <pubDate>Fri, 25 May 2018 09:41:43 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_deeputilities/</guid>
      <description>Introduction Processing data with tensorflow (python) could be accelerated with utilizing advanced CPU drivers and GPU. In this case, the pre-processing and post-processing may delay the fast GPU processing. For example, one of the most ordinary application is converting the segmentation mask picture to the multi-channel mask.
Matrix projecting As is shown below, in the segmentation picture, we use each color to represent a label. But in real application, to avoid the overlapping of the label value, we need to separate each label in one-hot format, i.</description>
    </item>
    
    <item>
      <title>FFmpeg Encoder Decoder for Python</title>
      <link>https://cainmagi.github.io/projects/python_ffmpegendecoder/</link>
      <pubDate>Thu, 24 May 2018 23:42:08 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/projects/python_ffmpegendecoder/</guid>
      <description>Introduction As a python-c-api implementation, this project allows users to use pure python codes to read/write video files as easily as calling open function series. Currently it supports the formats including mpeg2, h254, h265(hevc), vp8, vp9, etc.
By designing a dual-thread architecture, it could also demux the remote video stream including rtp, rstp, etc. Because it is used for analytical purposes, this project does not extract the audio stream in the video file/stream.</description>
    </item>
    
    <item>
      <title>Instructions for creating a Hugo site</title>
      <link>https://cainmagi.github.io/playground/20180524ichs/</link>
      <pubDate>Thu, 24 May 2018 16:13:10 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/playground/20180524ichs/</guid>
      <description>背景 这是一篇中文教程，基于前人的经验，补全一些没有提到的技巧。首先我们可以阅读以下站点的教程：
一篇基础的、关于如何使用Hugo创建新站点，并托管到Github的流程
补全教程：关于如何理解、使用Hugo主题
然而，实际编写站点内容没有固定的套路，很多情况下能使用哪些功能取决于对应的主题。譬如有的主题支持HTML5，我们就能实现一些对应的技巧；而有些相对简单的主题不支持，我们就只能实现一些基本功能。共通的一些技巧可以查阅：
Hugo官方文档
本站使用的模板是Forty，这个模板支持HTML5。在做好所有的基础设置以后，我们可以用MarkDown写任何一篇内容。关于更高级的编写内容的技巧，可以查阅模板自带的Element范例。这些高级技巧无一例外需要通过HTML语法实现，包括但不限于分栏 ，按钮 ，图标 ，表单 ，下标 或者上标如此等等……
补全 套用模板来定制主页 以Forty为例，下载后，其模板文档的关键结构如下：
forty |---archetypes |---exampleSite | |---archetypes | |---content | | `---blogs | | |---_index.md | | `---blog1.md | |---static | |---config.toml | `---netlify.toml |---images |---layouts |---static | |---img | `---elements.html `---theme.toml  我们可以看到，Element就在static目录下，它不是由MarkDown渲染出的，而是静态网页，我们需要的参考都只能通过HTML的形式获取。图像文件都保存在static/img下，但是这里有不只一个static文件夹，我们稍后再讨论。现在我们需要关注的是exampleSite，它是一个模板自带的范例站点，通过调用
hugo server --buildDrafts --watch  可以直接渲染出该范例，它就是模板提供预览的Demo。在这个Demo里面，主页是由config.toml定制的，我们需要将它和netlify.toml拷贝到我们自己的站点根目录下，覆盖原有的文件，通过上述命令，我们可以直接看到，主页的定制生效在了我们自己的站点上。
config.toml是用Golang编写的，但是作为配置文件，它对于新手并不难懂，并且有丰富的注释。从[params]开始，该文件开始定义主页存在的要素，包括但不限于顶部的导航条（导航菜单）、主页Banner，精美的导航图片模块、联系方式以及底格的社交媒体按钮。通过替换图片文件、修改模块参数等方式，我们可以逐渐将范例的内容替换成我们自己的。
特别注意的是使用图标的技巧。这一套图标是来自Font Awesome Icons的开源协议，通过以下的按钮，我们可以查阅所有我们能使用的图标：
Font Awesome Icons
内容的基本结构 让我们来重点关注根目录下content的内容
. |---archetypes |---content | |---blogs | | |---_index.</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://cainmagi.github.io/about/</link>
      <pubDate>Wed, 23 May 2018 16:49:20 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/about/</guid>
      <description>My Personal Statements A Ph.D student in University of Houston (UH). Interested area includes: machine learning, programming and religion.
Curriculum Vitae Click here to see my curriculum vitae created in 2018.
Curriculum Vitae
Click here to see my curriculum vitae created in 2017.
Curriculum Vitae
 Personal Statement
  Skill Coding  Python: Using tensorflow, python-c-api, PyQt5 and opencv3 to process data. Matlab: Be able to use several toolboxes and matlab-c-api.</description>
    </item>
    
  </channel>
</rss>