<!DOCTYPE HTML>
<html>
    <!-- Header -->
    <head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<meta name="description" content="A Ph.D student in University of Houston (UH). Interested area includes: machine learning, programming and religion.">
	<meta name="author" content="Yuchen Jin">
	
	<meta name="generator" content="Hugo 0.55.1" />
	<title>Special Notes on Aug. 24, 2018 &middot; Rosenkreutz Studio</title>
	<!-- Stylesheets -->
	
	<link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.3.1/semantic.min.css" rel="stylesheet" type="text/css">
	<link rel="stylesheet" type="text/css" href="https://cainmagi.github.io/css/tooltipster.bundle.min.css" />
	<link rel="stylesheet" type="text/css" href="https://cainmagi.github.io/css/tooltipster-sideTip-borderless.min.css" />
	<link rel="stylesheet" href="https://cainmagi.github.io/css/main.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/title.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/extensions.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/jq-images.css"/>
	
	

	

	<!-- Custom Fonts -->
	
	
	<link href="https://cainmagi.github.io/css/font-awesome.min.css" rel="stylesheet" type="text/css">
	<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

	
	<link rel="shortcut icon" type="image/x-icon" href="https://cainmagi.github.io/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://cainmagi.github.io/favicon.ico">
	

	<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
	<script src="js/ie/html5shiv.js"></script>
	<script src="js/ie/html5shiv.jsrespond.min.js"></script>
	<![endif]-->
</head>

    <body>

    <!-- Wrapper -->
    <div id="wrapper">

            <!-- Header -->
    <header id="header" class="alt">
        <a href="https://cainmagi.github.io/" class="logo"><strong>CainMagi</strong> <span>University of Houston</span></a>
        <nav>
            <a href="#menu">Menu</a>
        </nav>
    </header>

<!-- Menu -->
    <nav id="menu">
        <ul class="links">
            
                <li><a href="https://cainmagi.github.io/">Home</a></li>
            
                <li><a href="https://cainmagi.github.io/about">About</a></li>
            
                <li><a href="https://cainmagi.github.io/notes">Notes</a></li>
            
                <li><a href="https://cainmagi.github.io/researches">Researches</a></li>
            
                <li><a href="https://cainmagi.github.io/projects">Projects</a></li>
            
                <li><a href="https://cainmagi.github.io/playground">Playground</a></li>
            

        </ul>
        <ul class="actions vertical">
            
                <li><a href="http://welllogging.egr.uh.edu/" class="button special fit">Laboratory Page</a></li>
            
            
        </ul>
    </nav>

        <!-- Main -->
            <div id="main" class="alt">

                
                    <section id="one">
                        <div class="inner">
                            <header id="pagetitle" class="major">
                                <h1 id='main_title'>Special Notes on Aug. 24, 2018</h1>
                                <table class="sub-title">
                                    <tbody>
                                        <tr>
                                            <th>Date:</th>
                                            <td>Aug 24, 2018</td>
                                        </tr> 
                                        <tr>
                                            <th>Last Updated:</th>
                                            <td>Jan 29, 2019</td>
                                        </tr>
                                        <tr>
                                            <th>Categories:</th>
                                            <td><section class="dream-tags">
                                                    <a class="ui label categ" href="/categories/notes" title="Notes">Notes</a>
                                                    
                                                    <a class="ui label categ" href="/categories/theory" title="Theory">Theory</a>
                                                    
                                                
                                            </section></td>
                                        </tr>
                                        <tr>
                                            <th>Tags:</th>
                                            <td><section class="dream-tags">
                                                    <a class="ui label" href="/tags/algorithm" title="algorithm">algorithm</a>
                                                    
                                                    <a class="ui label" href="/tags/deep-learning" title="deep-learning">deep-learning</a>
                                                    
                                                    <a class="ui label" href="/tags/inverse-problem" title="inverse-problem">inverse-problem</a>
                                                    
                                                    <a class="ui label" href="/tags/optimization" title="optimization">optimization</a>
                                                    
                                                
                                            </section></td>
                                        </tr>
                                    </tbody>
                                </table>
                                
                                <span class="image main"><img src="/img/notes/special.jpg" alt="" /></span>
                                
                            </header>
                            
                            <hr/>
                            <h1 id="contents">Contents</h1>
                            <p><nav id="TableOfContents">
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#derivation-for-vector-valued-function">Derivation for vector-valued function</a></li>
<li><a href="#jacobian-matrix">Jacobian matrix</a></li>
<li><a href="#hessian-matrix">Hessian matrix</a></li>
</ul></li>
<li><a href="#derive-lma-from-a-second-order-expansion-view">Derive LMA from a second-order expansion view</a>
<ul>
<li><a href="#general-form">General form</a></li>
<li><a href="#lma-for-least-square-problem">LMA for least square problem</a></li>
</ul></li>
</ul>
</nav></p>
                            
                            <hr/>
                            

<h1 id="introduction">Introduction</h1>

<div class="box wiki">
    <div style="float: left;"> 
        <a href="https://en.wikipedia.org/wiki/Levenberg%e2%80%93Marquardt_algorithm" class="image"><img src="https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png" width="60px" /></a>
    </div>
    <div style="margin-left: 70px; font-style:normal;">
        <p>Check here to see <a href="https://en.wikipedia.org/wiki/Levenberg%e2%80%93Marquardt_algorithm"><b>Levenberg–Marquardt algorithm</b></a> in Wikipedia.</p>
    </div>
</div>

<p>Check this link and we could review the theory of Levenberg–Marquardt algorithm (LMA), which is used as an improvement compared to the plain first-order gradient descent method. In this short discussion, we would like to talk about how and why we could derive such a method. Then we would know that when we could use this method and when we could not.</p>

<p>To understand the following part, we need to introduce such concepts:</p>

<h2 id="derivation-for-vector-valued-function">Derivation for vector-valued function</h2>

<p>Suppose that we have a function $\mathbf{y} = \mathbf{f}(\mathbf{x})$, where $\mathbf{x}$ is an $M$ dimensional vector (or $1 \times M$ dimensional matrix) and $\mathbf{y}$ is an $N$ dimensional vector. Thus we know $\mathbf{f}$ is an $\mathbb{R}^{M} \rightarrow \mathbb{R}^{N}$ mapping. In particular, if $N=1$, the function accepts an $M$ dimensional vector but only gives one output value. We use $y = f(\mathbf{x})$ to represent this case. In another case, the function only accepts one value but outputs a vector. We use $\mathbf{y} = \mathbf{f}(x)$ to represent this case.</p>

<p>First, let us talk about the special case $f(\mathbf{x})$. Since the input of $f$ is a vector, we could view it as $f(x_1,~x_2,~\ldots~x_M)$, where $x_i$ is the $i^{\mathrm{th}}$ element of $\mathbf{x}$. We could calculate $\frac{\partial f}{\partial x_i}$ for any $i$. Thus we denote the derivative of $f$ over $\mathbf{x}$ is:</p>

<div class="overflow">
\begin{align}
    \frac{\partial f}{\partial \mathbf{x}} = \left[ \frac{\partial f}{\partial x_1} ,~ \frac{\partial f}{\partial x_2} ,~ \ldots ,~ \frac{\partial f}{\partial x_M} \right].
\end{align}
</div>

<p>In the other case $\mathbf{f}(x)$. Denote $f_i(x)$ as the $i^{\mathrm{th}}$ element of $\mathbf{y}$. Since we could calculate $\frac{\partial f_i}{\partial x}$ for any $i$. Thus we denote the derivative of $\mathbf{f}$ over $x$ is:</p>

<div class="overflow">
\begin{align}
    \frac{\partial \mathbf{f}}{\partial x} = \left[ \frac{\partial f_1}{\partial x} ,~ \frac{\partial f_2}{\partial x} ,~ \ldots ,~ \frac{\partial f_N}{\partial x} \right].
\end{align}
</div>

<h2 id="jacobian-matrix">Jacobian matrix</h2>

<div class="box wiki">
    <div style="float: left;"> 
        <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant#Jacobian_matrix" class="image"><img src="https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png" width="60px" /></a>
    </div>
    <div style="margin-left: 70px; font-style:normal;">
        <p>Check here to see <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant#Jacobian_matrix"><b>Jacobian matrix</b></a> in Wikipedia.</p>
    </div>
</div>

<p>After learning the above definitions, now we may want to define the derivative of $\mathbf{f}(\mathbf{x})$. Since it has multiple inputs and multiple outputs, the meaning of its derivative may not be very apparent. Generally we call such a derivative, or gradient, as &ldquo;<em>Jacobian matrix</em>&rdquo;. It has $N$ rows and $M$ columns and could be formulated as</p>

<div class="overflow">
\begin{align}
    \mathbf{J} = 
    \begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_{M}} \\
    \frac{\partial f_2}{\partial x_1} & \ddots & \cdots & \frac{\partial f_2}{\partial x_{M}} \\
    \vdots &   & \ddots & \vdots \\
    \frac{\partial f_{N}}{\partial x_1} & \cdots & \frac{\partial f_{N}}{\partial x_{M - 1}} & \frac{\partial f_{N}}{\partial x_{M}}
    \end{bmatrix}
\end{align}
</div>

<p>To understand why Jacobian matrix could be defined as the derivative, we need to suppose that we have a function composition $z = g(\mathbf{y}) = g(\mathbf{f}(\mathbf{x}))$. If we have already gotten the derivative over $\mathbf{y}$, i.e. we have known $\frac{\partial g}{\partial \mathbf{y}}$. Then we could derive $\frac{\partial g}{\partial \mathbf{x}}$ by</p>

<div class="overflow">
\begin{align}
    \frac{\partial g}{\partial \mathbf{x}} = \frac{\partial g}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \frac{\partial g}{\partial \mathbf{y}} \mathbf{J} .
\end{align}
</div>

<p>In this equitation, the Jacobian matrix serves as a weight matrix which re-weights the gradient over $\mathbf{y}$ and map the gradient from the space $\mathbb{R}^N$ to $\mathbb{R}^M$. If we use numerical method to calculate $\frac{\partial g}{\partial \mathbf{x}}$ and $\frac{\partial g}{\partial \mathbf{y}}$ respectively, we could find that the above equation is satisfied.</p>

<h2 id="hessian-matrix">Hessian matrix</h2>

<div class="box wiki">
    <div style="float: left;"> 
        <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" class="image"><img src="https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png" width="60px" /></a>
    </div>
    <div style="margin-left: 70px; font-style:normal;">
        <p>Check here to see <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization"><b>Newton&#39;s method in optimization</b></a> in Wikipedia.</p>
    </div>
</div>

<p>In some cases, the second-order derivative could help us to get a better or faster convergence. For example, the Newton&rsquo;s method shows the most plain idea for utilizing the second-order term in optimization. Compare to the gradient descent method (which is first-order method), it could get rid of the influence from the local gradient and take a shorter route by making use of the curvature.</p>

<div class="box wiki">
    <div style="float: left;"> 
        <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm" class="image"><img src="https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png" width="60px" /></a>
    </div>
    <div style="margin-left: 70px; font-style:normal;">
        <p>Check here to see <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm"><b>Gauss-Newton algorithm</b></a> in Wikipedia.</p>
    </div>
</div>

<p>Furthermore, the Gauss-Newton algorithm is a practical version of the Newton&rsquo;s method. By approximating the Hessian matrix (second-order derivative) as $\mathbf{J}^T\mathbf{J}$, it supposes to a good approximation for the second order term (in least square problem).</p>

<div class="box wiki">
    <div style="float: left;"> 
        <a href="https://en.wikipedia.org/wiki/Hessian_matrix" class="image"><img src="https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png" width="60px" /></a>
    </div>
    <div style="margin-left: 70px; font-style:normal;">
        <p>Check here to see <a href="https://en.wikipedia.org/wiki/Hessian_matrix"><b>Hessian matrix</b></a> in Wikipedia.</p>
    </div>
</div>

<p>First, suppose that there is a single valued function which accepts a vector $f(\mathbf{x})$. Then its corresponding Hessian matrix is</p>

<div class="overflow">
\begin{align}
    \mathbf{H}(f) = 
    \begin{bmatrix}
    \frac{\partial f}{\partial x_1^2} & \frac{\partial f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial f}{\partial x_1 \partial x_{M}} \\
    \frac{\partial f}{\partial x_2 \partial x_1} & \ddots & \cdots & \frac{\partial f}{\partial x_2 \partial x_{M}} \\
    \vdots &   & \ddots & \vdots \\
    \frac{\partial f}{\partial x_M \partial x_1} & \cdots & \frac{\partial f}{\partial x_M \partial x_{M - 1}} & \frac{\partial f}{\partial x_M \partial x_{M}}
    \end{bmatrix}.
\end{align}
</div>

<p>Frankly, in the case of function vector, which means we have $\mathbf{y} = \mathbf{f}(\mathbf{x})$. In this case, the Hessian matrix would become a 3D one, which means</p>

<div class="overflow">
\begin{align}
    \mathbf{H}(\mathbf{f}) = 
    \begin{bmatrix}
        \mathbf{H}(f_1) & \mathbf{H}(f_2) & \mathbf{H}(f_3) & \cdots & \mathbf{H}(f_N)
    \end{bmatrix}^T,
\end{align}
</div>

<p>where we denote that $\mathbf{f}(\mathbf{x}) = \begin{bmatrix} f_1(\mathbf{x}) &amp; f_2(\mathbf{x}) &amp; \cdots &amp; f_N(\mathbf{x}) \end{bmatrix}^T$. The element of the above 3D matrix is a 2D matrix.</p>

<h1 id="derive-lma-from-a-second-order-expansion-view">Derive LMA from a second-order expansion view</h1>

<p>In this part, we would derive the LMA from a second-order expansion view. In fact, LMA could be regarded as a semi-second-order gradient approach, because when calculating the derivative, we use the second-order expansion for the least square problem while the inner function, i.e. the forward function is only expanded in the first-order.</p>

<h2 id="general-form">General form</h2>

<p>Considering a function composition $g(\mathbf{f}(\mathbf{x}))$, where we denote the inner function vector as $\mathbf{z} = \mathbf{f}(\mathbf{x})$. Then we could calculate the incremental of the single valued function by</p>

<div class="overflow">
\begin{align}
    g(\mathbf{z} + \Delta \mathbf{z}) - g(\mathbf{z}) \approx \nabla g^T(\mathbf{z}) \Delta \mathbf{z} + \frac{1}{2} \Delta \mathbf{z}^T \mathbf{H}(g) \Delta \mathbf{z},
\end{align}
</div>

<p>which indicates that we expand the function $g$ in second order. Note that we use Hessian matrix $\mathbf{H}(g)$ to represent the second order derivative. Then we could represent the incremental $\Delta \mathbf{z}$ by</p>

<div class="overflow">
\begin{align}
    \Delta \mathbf{z} = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \Delta \mathbf{x} = \mathbf{J}(\mathbf{f}) \Delta \mathbf{x}.
\end{align}
</div>

<p>Note that we have used the Jacobian matrix $\mathbf{J}(\mathbf{f})$ to represent the derivative of function vector $\mathbf{f}$ over $\mathbf{x}$. Actually, we could expand $\mathbf{f}(\mathbf{x}+\Delta \mathbf{x})$ to the second order. But here we only use the first order term to represent the derivative. So the expansion for the inner function vector $\mathbf{f}$ is still remained in the first order.</p>

<h2 id="lma-for-least-square-problem">LMA for least square problem</h2>

<p>Considering the least square problem:</p>

<div class="overflow">
\begin{align} \label{fml:intro:misfit}
    \hat{\mathbf{x}} = \arg \min\limits_{\mathbf{x}} \lVert \mathbf{y} - \mathbf{f}(\mathbf{x}) \rVert^2,
\end{align}
</div>

<p>where $\mathbf{y}$ is constant (usually viewed as &ldquo;observation&rdquo;), and $\mathbf{x}$ is what we need to optimize.</p>

<p>So the least square problem could be described as finding a stationary point of a function composition, i.e.</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        g(\mathbf{z}) &= \lVert \mathbf{y} - \mathbf{z} \rVert^2, \\
        \mathbf{z} &= \mathbf{f}(\mathbf{x}), \\
        \nabla g^T(\mathbf{z}) &= - 2 (\mathbf{y} - \mathbf{z})^T, \\
        \mathbf{H}(g) &= 2 \mathbf{I}, \\
        \mathbf{J}(\mathbf{f}) &:= \mathbf{J},
    \end{aligned}
\end{equation}
</div>

<p>where we denote the Jacobian matrix of the inner function vector $\mathbf{f}$ as $\mathbf{J}$ for the convenience.</p>

<p>Then we could rewrite the incremental as</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        g(\mathbf{z} + \Delta \mathbf{z}) - g(\mathbf{z}) &\approx - 2 (\mathbf{y} - \mathbf{z})^T \Delta \mathbf{z} + \Delta \mathbf{z}^T \Delta \mathbf{z}, \\
        g(\mathbf{x} + \Delta \mathbf{x}) - g(\mathbf{x}) &\approx - 2 (\mathbf{y} - \mathbf{f}(\mathbf{x}))^T \mathbf{J} \Delta \mathbf{x} + \Delta \mathbf{x}^T \mathbf{J}^T \mathbf{J} \Delta \mathbf{x}, \\
        \frac{g(\mathbf{x} + \Delta \mathbf{x}) - g(\mathbf{x})}{ \Delta \mathbf{x} } &\approx \Delta \mathbf{x}^T \mathbf{J}^T \mathbf{J} - 2 (\mathbf{y} - \mathbf{f}(\mathbf{x}))^T \mathbf{J} = 0.
    \end{aligned}
\end{equation}
</div>

<p>Set the derivative to 0, therefore we could get</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        \mathbf{J}^T \mathbf{J} \Delta \mathbf{x} &= 2 \mathbf{J}^T (\mathbf{y} - \mathbf{f}(\mathbf{x})), \\
        \Delta \mathbf{x} &= 2 \left( \mathbf{J}^T \mathbf{J} \right)^{-1} \mathbf{J}^T (\mathbf{y} - \mathbf{f}(\mathbf{x})).
    \end{aligned}
\end{equation}
</div>

<p>Jacobian matrix is not a square matrix in most time. However, the symmetric matrix $\mathbf{J}^T \mathbf{J}$ is a square one. So we could calculate its inverse. This result also explains that why we use $\mathbf{J}^T\mathbf{J}$ as the approximation of Hessian matrix. To be frank, this technique could only be used in the problems like least square.</p>

<p>To prevent the cases where the inverse problem is underdetermined, we could add a &ldquo;damped vector&rdquo; which is derived from the pure first order gradient,</p>

<div class="overflow">
\begin{align}
    \Delta \mathbf{x} &= 2 \left( \mathbf{J}^T \mathbf{J} + \lambda \mathbf{I} \right)^{-1} \mathbf{J}^T (\mathbf{y} - \mathbf{f}(\mathbf{x})).
\end{align}
</div>

<p>Furthermore, there is an adaption that ensure the damped vector is scalable:</p>

<div class="overflow">
\begin{align}
    \Delta \mathbf{x} &= 2 \left( \mathbf{J}^T \mathbf{J} + \lambda \mathrm{diag} \left( \mathbf{J}^T \mathbf{J} \right) \right)^{-1} \mathbf{J}^T (\mathbf{y} - \mathbf{f}(\mathbf{x})).
\end{align}
</div>

<p>This solution is exactly what we call &ldquo;LMA&rdquo;.</p>

                        </div>
                    </section>
            <!-- Disqus Inject -->
                
                  <section>
    <div class="inner" id="disqus_thread"></div>
    <script type="text/javascript">

    (function() {
          
          
          if (window.location.hostname == "localhost")
                return;

          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          var disqus_shortname = 'rosenkreutz-studio';
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <div class="inner"><a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></div>
</section>
                
            </div>
            
        <!-- Footer -->
            
                <!-- Footer -->
    <footer id="footer">
        <div class="inner">
            <ul class="icons">
                
                    <li><a href="mailto:cainmagi@gmail.com" class="icon alt fa-envelope" target="_blank"><span class="label">Email</span></a></li>
                
                    <li><a href="https://weibo.com/u/5885093621" class="icon alt fa-weibo" target="_blank"><span class="label">Weibo</span></a></li>
                
                    <li><a href="https://github.com/cainmagi" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a></li>
                
                    <li><a href="https://steamcommunity.com/id/cainmagi" class="icon alt fa-steam" target="_blank"><span class="label">Steam</span></a></li>
                
                    <li><a href="https://www.youtube.com/channel/UCzqpNK5qFMy5_cI1i0Z1nQw" class="icon alt fa-youtube-play" target="_blank"><span class="label">Youtube</span></a></li>
                
                    <li><a href="https://music.163.com/#/user/home?id=276304206" class="icon alt fa-music" target="_blank"><span class="label">Netease Music</span></a></li>
                
            </ul>
            <ul class="copyright">
                <li>&copy; Well-logging laboratory, Department of Electrical and Computer Engineering, University of Houston</li>
                
            </ul>
        </div>
    </footer>

            
        </div>

    <!-- Scripts -->
        <!-- Scripts -->
    <!-- jQuery -->
    <script src="https://cainmagi.github.io/js/jquery.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.scrolly.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.scrollex.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.elevatezoom.js" type="text/javascript"></script>
    <script src="https://cainmagi.github.io/js/jquery.images.js"></script>
    <script src="https://cainmagi.github.io/js/skel.min.js"></script>
    <script src="https://cainmagi.github.io/js/util.js"></script>
    <script type="text/javascript" src="https://cainmagi.github.io/js/tooltipster.bundle.min.js"></script>

    

    <!-- Main JS -->
    <script src="https://cainmagi.github.io/js/main.js"></script>
    <script src="https://cainmagi.github.io/js/extensions.js"></script>
    
    
    <script src="https://cainmagi.github.io/js/title.js"></script>
    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-119875813-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    
    
    <script src="https://cainmagi.github.io/js/highlight.pack.js"></script>
    <link rel="stylesheet" href="https://cainmagi.github.io/css/vs2015adp.css">
    <script>hljs.initHighlightingOnLoad();</script>
    
    <script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
          extensions: ["AMSmath.js", "AMSsymbols.js", "boldsymbol.js", "color.js"]
      }
    }
  });

  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
    
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

    

    

    </body>
</html>
