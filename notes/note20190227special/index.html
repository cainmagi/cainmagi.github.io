<!DOCTYPE HTML>
<html>
    <!-- Header -->
    <head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<meta name="description" content="A Ph.D student in University of Houston (UH). Interested area includes: machine learning, programming and religion.">
	<meta name="author" content="Yuchen Jin">
	
	<meta name="generator" content="Hugo 0.55.1" />
	<title>Special Notes on Feb. 27, 2019 &middot; Rosenkreutz Studio</title>
	<!-- Stylesheets -->
	
	<link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.3.1/semantic.min.css" rel="stylesheet" type="text/css">
	<link rel="stylesheet" type="text/css" href="https://cainmagi.github.io/css/tooltipster.bundle.min.css" />
	<link rel="stylesheet" type="text/css" href="https://cainmagi.github.io/css/tooltipster-sideTip-borderless.min.css" />
	<link rel="stylesheet" href="https://cainmagi.github.io/css/main.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/title.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/extensions.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/jq-images.css"/>
	
	

	

	<!-- Custom Fonts -->
	
	
	<link href="https://cainmagi.github.io/css/font-awesome.min.css" rel="stylesheet" type="text/css">
	<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

	
	<link rel="shortcut icon" type="image/x-icon" href="https://cainmagi.github.io/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://cainmagi.github.io/favicon.ico">
	

	<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
	<script src="js/ie/html5shiv.js"></script>
	<script src="js/ie/html5shiv.jsrespond.min.js"></script>
	<![endif]-->
</head>

    <body>

    <!-- Wrapper -->
    <div id="wrapper">

            <!-- Header -->
    <header id="header" class="alt">
        <a href="https://cainmagi.github.io/" class="logo"><strong>CainMagi</strong> <span>University of Houston</span></a>
        <nav>
            <a href="#menu">Menu</a>
        </nav>
    </header>

<!-- Menu -->
    <nav id="menu">
        <ul class="links">
            
                <li><a href="https://cainmagi.github.io/">Home</a></li>
            
                <li><a href="https://cainmagi.github.io/about">About</a></li>
            
                <li><a href="https://cainmagi.github.io/notes">Notes</a></li>
            
                <li><a href="https://cainmagi.github.io/researches">Researches</a></li>
            
                <li><a href="https://cainmagi.github.io/projects">Projects</a></li>
            
                <li><a href="https://cainmagi.github.io/playground">Playground</a></li>
            

        </ul>
        <ul class="actions vertical">
            
                <li><a href="http://welllogging.egr.uh.edu/" class="button special fit">Laboratory Page</a></li>
            
            
        </ul>
    </nav>

        <!-- Main -->
            <div id="main" class="alt">

                
                    <section id="one">
                        <div class="inner">
                            <header id="pagetitle" class="major">
                                <h1 id='main_title'>Special Notes on Feb. 27, 2019</h1>
                                <table class="sub-title">
                                    <tbody>
                                        <tr>
                                            <th>Date:</th>
                                            <td>Feb 27, 2019</td>
                                        </tr> 
                                        <tr>
                                            <th>Last Updated:</th>
                                            <td>Mar 4, 2019</td>
                                        </tr>
                                        <tr>
                                            <th>Categories:</th>
                                            <td><section class="dream-tags">
                                                    <a class="ui label categ" href="/categories/notes" title="Notes">Notes</a>
                                                    
                                                    <a class="ui label categ" href="/categories/papers" title="Papers">Papers</a>
                                                    
                                                
                                            </section></td>
                                        </tr>
                                        <tr>
                                            <th>Tags:</th>
                                            <td><section class="dream-tags">
                                                    <a class="ui label" href="/tags/research" title="research">research</a>
                                                    
                                                    <a class="ui label" href="/tags/deep-learning" title="deep-learning">deep-learning</a>
                                                    
                                                    <a class="ui label" href="/tags/algorithm" title="algorithm">algorithm</a>
                                                    
                                                    <a class="ui label" href="/tags/inverse-problem" title="inverse-problem">inverse-problem</a>
                                                    
                                                    <a class="ui label" href="/tags/optimization" title="optimization">optimization</a>
                                                    
                                                
                                            </section></td>
                                        </tr>
                                    </tbody>
                                </table>
                                
                                <span class="image main"><img src="/img/notes/special.jpg" alt="" /></span>
                                
                            </header>
                            
                            <hr/>
                            <h1 id="contents">Contents</h1>
                            <p><nav id="TableOfContents">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#the-issue">The issue</a>
<ul>
<li><a href="#non-convex-l-sub-q-sub-norm">Non-convex L<sub>q</sub> norm</a></li>
<li><a href="#convolutional-neural-network">Convolutional neural network</a></li>
<li><a href="#nett-regularization">NETT regularization</a>
<ul>
<li><a href="#well-posedness-and-weak-convergence">Well-posedness and weak convergence</a>
<ul>
<li><a href="#theorem-1">Theorem 1</a></li>
</ul></li>
<li><a href="#strong-convergence-and-non-linearity">Strong convergence and non-linearity</a></li>
</ul></li>
</ul></li>
<li><a href="#slides">Slides</a></li>
</ul>
</nav></p>
                            
                            <hr/>
                            

<div class="box" style="color:#FAA; padding-bottom:40px">
  <div style="float: left;"> 
    <p><i class="fa fa-exclamation-circle" aria-hidden="true" style="font-size: 50px"></i></p>
  </div>
  <div style="margin-left: 60px; font-style:normal;">
    <p>This article is incomplete now. Due to the problem of comprehension, I decide not to continue to review this article. Maybe I would try to review it again in the future.</p>
  </div>
</div>

<h1 id="introduction">Introduction</h1>

<p>In this topic, we are learning an article for studying the theory of the convergence for the neural network applied on inverse problem. To be specific, the related articles include</p>

<div class="box">
  <div class="row">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
      <p><strong id="ref-1" class="wrap_offset">[1]: </strong><i>NETT: Solving Inverse Problems with Deep Neural Networks</i>:</p>
      <p><a href="https://arxiv.org/pdf/1803.00092.pdf" class="button icon fa-file-pdf-o">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
      <p><strong id="ref-2" class="wrap_offset">[2]: </strong><i>Generalized Bregman distances and convergence rates for non-convex regularization methods</i>:</p>
      <p><a href="https://iopscience.iop.org/article/10.1088/0266-5611/26/11/115014/meta" class="button icon fa-file-pdf-o">Reference</a></p>
    </div>
  </div>
  <div class="row">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
      <p><strong id="ref-3" class="wrap_offset">[3]: </strong><i>On Total Convexity, Bregman Projections and Stability in Banach Spaces</i>:</p>
      <p><a href="http://www.heldermann.de/JCA/JCA11/jca11001.htm" class="button icon fa-file-pdf-o">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
      <p><strong id="ref-4" class="wrap_offset">[4]: </strong><i>Solving ill-posed inverse problems using iterative deep neural networks</i>:</p>
      <p><a href="https://iopscience.iop.org/article/10.1088/1361-6420/aa9581/meta" class="button icon fa-file-pdf-o">Reference</a></p>
    </div>
  </div>
</div>

<p>Among these references, <a href="#ref-1">[1]</a> is what we are mainly inspecting on. <a href="#ref-2">[2]</a> and <a href="#ref-3">[3]</a> are important supports for the theory. <a href="#ref-4">[4]</a> is another paper which is more concentrated on application. We are trying to understand the theory of deep learning approaches for solving inverse problem in the notes. Since most of these papers are written in mathematically academic style, I could not ensure that all of my notes are coherent to the original works.</p>

<p>In the following parts, I would discuss about the issue according to the structure of <a href="#ref-1">[1]</a>. In some sub-topics, I may introduce some extra theories from other articles. If we do not attach any notation, then it means this part is mainly from <a href="#ref-1">[1]</a>.</p>

<h1 id="the-issue">The issue</h1>

<p>In this article, we define the problem as</p>

<div class="overflow">
\begin{equation}
  \begin{aligned}
    x_+ \in \arg\min\limits_{x \in \mathbb{D}}~&\mathcal{T}_{\alpha,~y_{\delta}}(x), \\
    &\mathcal{T}_{\alpha,~y_{\delta}}(x) := \mathcal{D} ( \mathbf{F}(x),~ y_{\delta} ) + \alpha \mathcal{R} (\mathbb{V},~ x),
  \end{aligned}
\end{equation}
</div>

<p>where $\mathcal{D}$ is the distance function which could be L<sub>1</sub> norm, L<sub>2</sub> norm, or KL divergence, and $\mathcal{R}$ is the regularization function which could be L<sub>q</sub> norm, or a deep network. $\mathbb{V}$ defines a space of learnable variables. In <a class="icon" href="#mjx-eqn-1">$(1)$</a>, we assume that the the observed data $y_{\delta} = \mathbf{F}(x) + \xi_{\delta}$, which means the data has additive noise. However, if the data has no noise, i.e. the solution for $\mathbf{F}(x) = y_0$ exists, then we could have another form of the solution:</p>

<div class="overflow">
\begin{align} \label{fml:iss:int2}
  x_+ \in \arg\min \{ \mathcal{R}(\mathbb{V},~ x) | x \in \mathbb{D} \cap \mathbf{F}(x) = y_0 \}.
\end{align}
</div>

<p>This paper is focus on the proof of convergence for using appropriate regularizer $\mathcal{R}$, which could be either predefined function or a learnable neural network. Here both of these two cases are discussed.</p>

<h2 id="non-convex-l-sub-q-sub-norm">Non-convex L<sub>q</sub> norm</h2>

<p>We could use a non-convex L<sub>q</sub> norm which is based on basis expansion:</p>

<div class="overflow">
\begin{align}
  \mathcal{R}(\mathbb{V},~ \mathbf{x}) = \left| \mathbf{V} \boldsymbol{\Phi} \mathbf{x} \right|^q_q = \sum_{\lambda=1}^N v_{\lambda} | \boldsymbol{\varphi}^T_{\lambda} \mathbf{x} |^q,
\end{align}
</div>

<p>where $\mathbf{V}$ is a diagonal matrix which weights the basis matrix $\boldsymbol{\Phi}$. Consider we are using N basis, then $\boldsymbol{\varphi}_{\lambda}$ is a row of the matrix $\boldsymbol{\Phi}$.If $\boldsymbol{\Phi}$ is fixed, then we know that $\mathbf{V} \in \mathbb{V}$. If we apply some techniques like dictionary learning, then $\mathbf{V},~\boldsymbol{\Phi} \in \mathbb{V}$.</p>

<h2 id="convolutional-neural-network">Convolutional neural network</h2>

<p>A general convolutional neural network (CNN) $\boldsymbol{\Phi}(\mathbb{V},~x)$ could be formulated as a composition of a series of operators</p>

<div class="overflow">
\begin{align}
  \boldsymbol{\Phi}(\mathbb{V},~x) &:= (\sigma_L \circ \mathbb{V}_L \circ \sigma_{L-1} \circ \mathbb{V}_{L-1} \circ \cdots \circ \sigma_1 \circ \mathbb{V}_1 ) (x),\\
  \mathbb{V}_l &:= \mathbb{A}_l (x) + b_l, \\
  \mathbb{A}_{l,~\lambda} (x) &:= \sum_{\mu=1}^{N^{(l-1)}} \mathcal{K}^{(l)}_{\mu,~\lambda} (x_{\mu}).
\end{align}
</div>

<p>For any $l$, here we call $\sigma_l \circ \mathbb{V}_l$ &ldquo;a layer of the network&rdquo;. $\sigma_l$ is a nonlinear operator which could be ReLU, PReLU, or some other functions. $\mathbb{V}_l$ represents the affine linear operator of the layer, where $\mathbb{A}_l$ is defined by a series of convolutional functions $\mathcal{K}$. Assigning that there are $N^{(l)}$ channels for the l<sup>th</sup> layer, we know that $\lambda \in \mathbb{N} \cap [1,~N^{(l)}]$. Hence $\mathbb{A}$ maps the channels from the previous layer to the current one, while there are $N^{(l)}$ values in vector $b_l$ which is used to adjust the bias of the current layer.</p>

<p>If we use the network as the regularizer, then we have</p>

<div class="overflow">
\begin{align}
  \mathcal{R}(\mathbb{V},~x) = \psi ( \boldsymbol{\Phi}(\mathbb{V},~x) ).
\end{align}
</div>

<h2 id="nett-regularization">NETT regularization</h2>

<p>To understand this part, we may need to introduce the following definitions about topology:</p>

<div class="box wiki">
    <div style="float: left;"> 
        <a href="https://en.wikipedia.org/wiki/Coercive_function#Norm-coercive_mappings" class="image"><img src="https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png" width="60px" /></a>
    </div>
    <div style="margin-left: 70px; font-style:normal;">
        <p>Check here to see <a href="https://en.wikipedia.org/wiki/Coercive_function#Norm-coercive_mappings"><b>Norm-coercive mappings</b></a> in Wikipedia.</p>
    </div>
</div>

<blockquote class="">


  <p><strong>Coercive function</strong>:</p>

<p>A function $f$ is coercive means, when $\lVert x \rVert \rightarrow \infty$, we have $\lVert f(x) \rVert \rightarrow \infty$.</p>




</blockquote>

<blockquote class="">


  <p><strong>λ open set</strong>:</p></p>

<p>Consider a set $A$, then we have</p>

<ol>
<li>For any $B \supseteq A$, if $A$ equals to the intersection of all $B$, we call $A$ the $\Lambda$ set.</li></li>
<li>If $A$ could be represented by $A = B \cap C$, where $B$ is a lambda set and $C$ is a closed set, then we say $A$ is a λ-closed set.</li></li>
<li>If $B$ is a λ-closed set, then for a space $\Omega$, $A = \Omega \backslash B$ is a λ-open set.</li></li>
</ol>




</blockquote>

<div class="box wiki">
    <div style="float: left;"> 
        <a href="https://en.wikipedia.org/wiki/Closure_%28topology%29" class="image"><img src="https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png" width="60px" /></a>
    </div>
    <div style="margin-left: 70px; font-style:normal;">
        <p>Check here to see <a href="https://en.wikipedia.org/wiki/Closure_%28topology%29"><b>Closure (topology)</b></a> in Wikipedia.</p>
    </div>
</div>

<blockquote class="">


  <p><strong>Weakly continuous</strong>:</p>

<p>A function $f$ is weakly continuous if</p>

<ol>
<li>For any $x$, there exists a λ-open set $U$ that $x \in U$ and a open set $V$ that $f(x) \in V$.</li></li>
<li>Consider that the closure of $V$ is $\mathrm{Cl}(V)$. If $f(U) \subset \mathrm{Cl}(V)$, then we say $f$ is weekly λ-continuous at $x$.</li></li>
<li>For any $x$, if $f$ is weekly λ-continuous at $x$, then we say $f$ is weekly continuos.</li></li>
</ol>




</blockquote>

<p>To read further materials about the concepts related to topology, you may check the following link:</p>

<div class="box">
  <p><strong id="ref-5" class="wrap_offset">[5]: </strong><i>Weakly λ-continuous functions</i>:</p>
  <p><a href="http://www.kurims.kyoto-u.ac.jp/EMIS/journals/NSJOM/Papers/38_2/NSJOM_38_2_047_056.pdf" class="button icon fa-file-pdf-o">Reference</a></p>
</div>

<p>Here we would introduce another concept</p>

<blockquote>
    <p><b>Lower semi-continuous</b>:</p>
    <div class="row">
      <div class="6u 12u(mobilep)" style="margin-bottom:0">
        <p>A function is lower semi-continuous when there exists countable points we are lower continuous ($x_0 = \lim\limits_{x \rightarrow x_{0-}} x$), while other points are continuous. Here we use a figure to show this concept.</p>
      </div>
      <div class="6u 12u(mobilep)" style="margin-bottom:0">
        <img src="./Lower-semi.svg", >
      </div>
    </div>
</blockquote>

<h3 id="well-posedness-and-weak-convergence">Well-posedness and weak convergence</h3>

<div class="box" style="color:#6CF; padding-bottom:40px">
  <div style="float: left;"> 
    <p><i class="fa fa-info-circle" aria-hidden="true" style="font-size: 50px"></i></p>
  </div>
  <div style="margin-left: 60px; font-style:normal;">
    <p>Now I have no idea on deriving this conclusion, because the original texts in the paper is ambiguous. As a compensation, I would try to finish this part in the future.</p>
  </div>
</div>

<p>Consider a regularizer $\mathcal{R}(\mathbb{V},~\cdot)=\psi(\boldsymbol{\Phi}(\mathbb{V},~\cdot))$ which is defined by a CNN $\boldsymbol{\Phi}(\mathbb{V},~x)$. Here we would like to show a group of conditions:</p>

<div class="box">

  <ol>
<li>The regularizer $\mathcal{R}$ satisfies that:

<ul>
<li>For layer $l$, we have $\mathbb{A}_l (x)$ is bounded linear, i.e. $\exists~c_l,~\lVert x \rVert \leqslant c_l \lVert \mathbb{A}(x) \rVert$;</li>
<li>$\sigma_l$ is a weekly continuous and coercive function.</li>
<li>$\psi$ is a lower semi-continuous and coercive function.</li>
</ul></li>
<li>The data consistency term should satisfied that

<ul>
<li>For a $\tau \geqslant 1$, we have for any $y_1,~y_2,~y$, $\mathcal{D}(y_1,~y_2) \leqslant \tau \left( \mathcal{D}(y_1,~y) + \mathcal{D}(y,~y_2) \right)$.</li>
<li>For any $y,~y_0$, $\mathcal{D}(y,~y_0)=0~ \Longleftrightarrow~ y=y_0$.</li>
<li>For any $(y_k)_{k \in \mathbb{N}}$, $y_k \rightarrow y$~ \Longrightarrow~ \mathcal{D}(y_k,~y) \rightarrow 0$.</li>
<li>$\mathcal{D}(\mathbf{F}(x),~y)$ is sequentially lower semi-continuous with respect to $x$.</li>
</ul></li>
</ol>

</div>

<p>We know that from the condition 1, we could derive the following conditions <span class='popsym' title="Indeed I do not know how we could get this conclusion, but we may need to learn [2], which is also difficult to read."></span>:</p>

<div class="box">

  <ul>
<li>$\mathcal{R}$ is weakly and sequentially lower semi-continuous.</li>
<li>Since $\mathcal{R}$ is coercive, for all $t &gt; 0,~ \alpha &gt; 0$ and $y$, we have that $\{ x | \mathcal{T}_{\alpha,~y}(x) \leqslant t \}$ is sequentially weakly (closed and bounded).</li>
</ul>

</div>

<p>From the above conditions, then we could know that <span class='popsym' title="This derivation is also needed to be studied by reading [2]."></span>:</p>

<h4 id="theorem-1">Theorem 1</h4>

<blockquote class="">


  <p><strong>Well-posedness of CNN-regularization</strong>:</p>

<p>If the conditions 1,2 are satisfied, we would know that</p>

<ol>
<li><p>For all $\alpha &gt; 0$ and $y$, $\exists~\mathcal{T}_{\alpha,~y}(x)$.</p></li>

<li><p>If $y_k \rightarrow y$ and $x_k \in \arg \min \mathcal{T}_{\alpha,~y}(x)$, there exists the weak accumulation points $(x_k)_{k \in \mathbb{N}}$ which is derived from the minimizer $\mathcal{T}_{\alpha,~y}(x)$.</p></li>

<li><p>Consider $y = \mathbf{F}(x)$, we have $(y_k)_{k \in \mathbb{N}}$ that $\mathcal{D}(y_k,~y) + \mathcal{D}(y,~y_k) \leqslant \delta_k$, where $(\delta_k)_{k \in \mathbb{N}} \rightarrow 0$. Suppose that $x_k$ is k<sup>th</sup> iterative solution, i.e. $x_k \in \arg \min \mathcal{T}_{\alpha,~\delta_k}(x,~y_k)$. Then we could choose $\alpha$ that</p></li>
</ol>

<div class="overflow">
\begin{align}
  \lim\limits_{\delta \rightarrow 0} \alpha (\delta) = \lim\limits_{\delta \rightarrow 0} \frac{ \delta }{ \alpha (\delta) } = 0.
\end{align}
</div>

<p>Then we have such conclusions <span class='popsym' title="I could not understand why we get these conclusions."></span></p>

<ol>
<li><p>Weak accumulation points $(x_k)_{k \in \mathbb{N}}$ are the solution of $\eqref{fml:iss:int2}$.</p></li>

<li><p>$(x_k)_{k \in \mathbb{N}}$ has at least one weak accumulation point $x_+$.</p></li>

<li><p>Any weakly convergent subsequence $(x_k)_{k \in \mathbb{N}}$ satisfies $\mathcal{R}(\mathbb{V},~x_{k(n)}) \rightarrow \mathcal{R}(\mathbb{V},~x_+)$.</p></li>

<li><p>If the solution of $\eqref{fml:iss:int2}$ (also the step 1) is unique, then we know $(x_k) \rightarrow x_+$.</p></li>
</ol>




</blockquote>

<h3 id="strong-convergence-and-non-linearity">Strong convergence and non-linearity</h3>

<div class="box" style="color:#FB8; padding-bottom:40px">
  <div style="float: left;"> 
    <p><i class="fa fa-exclamation-triangle" aria-hidden="true" style="font-size: 50px"></i></p>
  </div>
  <div style="margin-left: 60px; font-style:normal;">
    <p>This article is still being produced. Please wait for several days to see the full edition.</p>
  </div>
</div>

<h1 id="slides">Slides</h1>

<p>View the slides here:</p>

<div class="youtube container">
    <object class="docs" data="https://cainmagi.github.io/notes/note20190227special/yjin4-week5-03012019.pdf" type="application/pdf">
        <div class="box center">
            <p style="top:10%;color:#FCC; margin-bottom:0.2em"><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> Ooops! Your browser does not support viewing pdfs.</p> 
            <p style="top:50%;text-align:center;height:100%; display:block"><a href="https://cainmagi.github.io/notes/note20190227special/yjin4-week5-03012019.pdf" class="button icon fa-pdf-o">Download PDF</a></p>
        </div>
    </object>
</div>

                        </div>
                    </section>
            <!-- Disqus Inject -->
                
                  <section>
    <div class="inner" id="disqus_thread"></div>
    <script type="text/javascript">

    (function() {
          
          
          if (window.location.hostname == "localhost")
                return;

          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          var disqus_shortname = 'rosenkreutz-studio';
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <div class="inner"><a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></div>
</section>
                
            </div>
            
        <!-- Footer -->
            
                <!-- Footer -->
    <footer id="footer">
        <div class="inner">
            <ul class="icons">
                
                    <li><a href="mailto:cainmagi@gmail.com" class="icon alt fa-envelope" target="_blank"><span class="label">Email</span></a></li>
                
                    <li><a href="https://weibo.com/u/5885093621" class="icon alt fa-weibo" target="_blank"><span class="label">Weibo</span></a></li>
                
                    <li><a href="https://github.com/cainmagi" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a></li>
                
                    <li><a href="https://steamcommunity.com/id/cainmagi" class="icon alt fa-steam" target="_blank"><span class="label">Steam</span></a></li>
                
                    <li><a href="https://www.youtube.com/channel/UCzqpNK5qFMy5_cI1i0Z1nQw" class="icon alt fa-youtube-play" target="_blank"><span class="label">Youtube</span></a></li>
                
                    <li><a href="https://music.163.com/#/user/home?id=276304206" class="icon alt fa-music" target="_blank"><span class="label">Netease Music</span></a></li>
                
            </ul>
            <ul class="copyright">
                <li>&copy; Well-logging laboratory, Department of Electrical and Computer Engineering, University of Houston</li>
                
            </ul>
        </div>
    </footer>

            
        </div>

    <!-- Scripts -->
        <!-- Scripts -->
    <!-- jQuery -->
    <script src="https://cainmagi.github.io/js/jquery.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.scrolly.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.scrollex.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.elevatezoom.js" type="text/javascript"></script>
    <script src="https://cainmagi.github.io/js/jquery.images.js"></script>
    <script src="https://cainmagi.github.io/js/skel.min.js"></script>
    <script src="https://cainmagi.github.io/js/util.js"></script>
    <script type="text/javascript" src="https://cainmagi.github.io/js/tooltipster.bundle.min.js"></script>

    

    <!-- Main JS -->
    <script src="https://cainmagi.github.io/js/main.js"></script>
    <script src="https://cainmagi.github.io/js/extensions.js"></script>
    
    
    <script src="https://cainmagi.github.io/js/title.js"></script>
    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-119875813-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    
    
    <script src="https://cainmagi.github.io/js/highlight.pack.js"></script>
    <link rel="stylesheet" href="https://cainmagi.github.io/css/vs2015adp.css">
    <script>hljs.initHighlightingOnLoad();</script>
    
    <script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
          extensions: ["AMSmath.js", "AMSsymbols.js", "boldsymbol.js", "color.js"]
      }
    }
  });

  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
    
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

    

    

    </body>
</html>
