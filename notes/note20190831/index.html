<!DOCTYPE HTML>
<html>
    <!-- Header -->
    <head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<meta name="description" content="A Ph.D student in University of Houston (UH). Interested area includes: machine learning, programming and religion.">
	<meta name="author" content="Yuchen Jin">
	
	<meta name="generator" content="Hugo 0.55.1" />
	<title>Note on Aug. 31, 2019 &middot; Rosenkreutz Studio</title>
	<!-- Stylesheets -->
	
	<link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.3.1/semantic.min.css" rel="stylesheet" type="text/css">
	<link rel="stylesheet" type="text/css" href="https://cainmagi.github.io/css/tooltipster.bundle.min.css" />
	<link rel="stylesheet" type="text/css" href="https://cainmagi.github.io/css/tooltipster-sideTip-borderless.min.css" />
	<link rel="stylesheet" href="https://cainmagi.github.io/css/main.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/title.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/extensions.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/jq-images.css"/>
	
	

	

	<!-- Custom Fonts -->
	
	
	<link href="https://cainmagi.github.io/css/font-awesome.min.css" rel="stylesheet" type="text/css">
	<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

	
	<link rel="shortcut icon" type="image/x-icon" href="https://cainmagi.github.io/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://cainmagi.github.io/favicon.ico">
	

	<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
	<script src="js/ie/html5shiv.js"></script>
	<script src="js/ie/html5shiv.jsrespond.min.js"></script>
	<![endif]-->
</head>

    <body>

    <!-- Wrapper -->
    <div id="wrapper">

            <!-- Header -->
    <header id="header" class="alt">
        <a href="https://cainmagi.github.io/" class="logo"><strong>CainMagi</strong> <span>University of Houston</span></a>
        <nav>
            <a href="#menu">Menu</a>
        </nav>
    </header>

<!-- Menu -->
    <nav id="menu">
        <ul class="links">
            
                <li><a href="https://cainmagi.github.io/">Home</a></li>
            
                <li><a href="https://cainmagi.github.io/about">About</a></li>
            
                <li><a href="https://cainmagi.github.io/notes">Notes</a></li>
            
                <li><a href="https://cainmagi.github.io/researches">Researches</a></li>
            
                <li><a href="https://cainmagi.github.io/projects">Projects</a></li>
            
                <li><a href="https://cainmagi.github.io/playground">Playground</a></li>
            

        </ul>
        <ul class="actions vertical">
            
                <li><a href="http://welllogging.egr.uh.edu/" class="button special fit">Laboratory Page</a></li>
            
            
        </ul>
    </nav>

        <!-- Main -->
            <div id="main" class="alt">

                
                    <section id="one">
                        <div class="inner">
                            <header id="pagetitle" class="major">
                                <h1 id='main_title'>Note on Aug. 31, 2019</h1>
                                <table class="sub-title">
                                    <tbody>
                                        <tr>
                                            <th>Date:</th>
                                            <td>Aug 31, 2019</td>
                                        </tr> 
                                        <tr>
                                            <th>Last Updated:</th>
                                            <td>Sep 3, 2019</td>
                                        </tr>
                                        <tr>
                                            <th>Categories:</th>
                                            <td><section class="dream-tags">
                                                    <a class="ui label categ" href="/categories/notes" title="Notes">Notes</a>
                                                    
                                                    <a class="ui label categ" href="/categories/papers" title="Papers">Papers</a>
                                                    
                                                
                                            </section></td>
                                        </tr>
                                        <tr>
                                            <th>Tags:</th>
                                            <td><section class="dream-tags">
                                                    <a class="ui label" href="/tags/research" title="research">research</a>
                                                    
                                                    <a class="ui label" href="/tags/optimization" title="optimization">optimization</a>
                                                    
                                                    <a class="ui label" href="/tags/algorithm" title="algorithm">algorithm</a>
                                                    
                                                
                                            </section></td>
                                        </tr>
                                    </tbody>
                                </table>
                                
                                <span class="image main"><img src="/img/notes/default.jpg" alt="" /></span>
                                
                            </header>
                            
                            <hr/>
                            <h1 id="contents">Contents</h1>
                            <p><nav id="TableOfContents">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#theory">Theory</a>
<ul>
<li><a href="#abstract-framework-for-adaptive-algorithms">Abstract framework for adaptive algorithms</a></li>
<li><a href="#non-convergence-of-adam">Non-convergence of Adam</a>
<ul>
<li><a href="#start-from-a-strong-assumption">Start from a strong assumption</a></li>
<li><a href="#generalize-the-proof-in-online-cases">Generalize the proof in online cases</a></li>
</ul></li>
</ul></li>
</ul>
</nav></p>
                            
                            <hr/>
                            

<h1 id="introduction">Introduction</h1>

<p>In the <a href="../note20190516">notes on May 16</a>, we have already talked about the Adam algorithm. Combining the first-order momentum and the adaptive learning rate, Adam performs the normalization on the gradients. When the gradient is too strong, Adam correct it by reducing the learning rate, vice versa. In the aforementioned notes, we have explained why Adam is designed like this and pointed out that the proof of Adam is not correct. In this topic, we would talk about another paper, which is aimed at correcting the problem of Adam and giving a correct proof. The paper where we discuss the theory and proof is in the following paper:</p>

<p><a href="hhttps://openreview.net/forum?id=ryQu7f-RZ" class="button icon fa-file-pdf-o">Reference</a></p>

<p>In this article, the author proposes tha Adam would not get converged in some cases. In the introduction part, the author points out that <strong>the existing adaptive methods are easy to eliminate the influences from the minibatches with large gradients</strong>, which makes those algorithms would not get converged on a convex problem. As an alternative, the proposed algorithm, <strong>AMSGrad</strong>, has solved such a problem. It could be summarized as below:</p>


<div class="box">

  <ul>
<li>Denotation:

<ul>
<li>$T$: iteration number;</li>
<li>$\alpha_t$: step size on time $t$;</li>
<li>$\delta$: small indefinite amount that is usually 1e<sup>-8</sup>;</li>
<li>$\rho_1,~\rho_2$: two decay rates;</li>
<li>$\mathbf{s}$: momentum which is initialized as $\mathbf{0}$;</li>
<li>$\mathbf{r}$: adaptive learning rate which is initialized as a diagonal matrix $\mathrm{diag}(\mathbf{0})$.</li>
<li>$\tilde{\mathbf{r}}$: corrected adaptive learning rate which is initialized as a diagonal matrix $\mathrm{diag}(\mathbf{0})$.</li>
</ul></li>
<li>Then in each iteration $t={1,~2,~3,~\cdots,~T}$, we have

<ol>
<li>Pick m samples randomly from a set (or pick those samples in sequence from a randomly shuffled set). We call the m samples $(\mathbf{x}_k,~\mathbf{y}_k)$ as a batch;</li>
<li>Calculate the gradient $\mathbf{g} = \frac{1}{m} \nabla_{\boldsymbol{\Theta}} \sum\limits_{k=1}^m \mathcal{L} \left( \mathbf{y}_k,~ \mathcal{D}_{\boldsymbol{\Theta}} (\mathbf{x}_k) \right)$;</li>
<li>Update the momentum by $\mathbf{s} \leftarrow \rho_1 \mathbf{s} + (1 - \rho_1) \mathbf{g}$;</li>
<li>Update the adaptive learning rate by $\mathbf{r} \leftarrow \rho_2 \mathbf{r} + (1 - \rho_2) \mathrm{diag}(\mathbf{g})^2$;</li>
<li>$\dagger$ Correct the adaptive learning rate by $\tilde{\mathbf{r}} \leftarrow \max \left( \tilde{\mathbf{r}}, \mathbf{r} \right)$;</li>
<li>Correct the initialization bias by $\hat{\mathbf{s}} \leftarrow \dfrac{\mathbf{s}}{1 - \rho_1^t}$, $\hat{\mathbf{r}} \leftarrow \dfrac{\tilde{\mathbf{r}}}{1 - \rho_2^t}$;</li>
<li>Update parameters by $\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} - \dfrac{ \epsilon }{\delta + \sqrt{\hat{\mathbf{r}}}} \hat{\mathbf{s}}$.</li>
</ol></li>
</ul>

<p>$\dagger$: This is the modification which makes AMSGrad different from Adam. Since it uses the maximal $\mathbf{v}$ to update the learning rate, the gradient direction is guided by the momentum more.

</div>


<h1 id="theory">Theory</h1>

<h2 id="abstract-framework-for-adaptive-algorithms">Abstract framework for adaptive algorithms</h2>

<p>The previous adaptive methods are though to have a common abstract framework. We use $\phi$ to represent the momentum function and $\psi$ to represent the adaptive learning rate function. The framework could be described as:</p>

<table>
<thead>
<tr>
<th>Abstract framework of adaptive algorithms</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="./adaptive.svg" alt="" /></td>
</tr>
</tbody>
</table>

<p>Such a frame work would be applied on almost all kinds of optimizers in deep learning. After defining the specific $\phi$ and $\psi$, it would becomes a concrete learning algorithm.</p>


<div class="mintable">

  <table>
<thead>
<tr>
<th>Algorithm</th>
<th>$\phi$</th>
<th>$\psi$</th>
</tr>
</thead>

<tbody>
<tr>
<td>SGD</td>
<td>$g_t$</td>
<td>$\mathbf{I}$</td>
</tr>

<tr>
<td>AdaGrad</td>
<td>$g_t$</td>
<td>$\sum_{i=1}^t \left( \mathrm{diag}(g_i)^2 \right)$</td>
</tr>

<tr>
<td>RMSProp</td>
<td>$g_t$</td>
<td>$\left( 1 - \beta\right) \sum_{i=1}^t \left( \beta^{t-i}\mathrm{diag}(g_i)^2 \right)$</td>
</tr>

<tr>
<td>Adam</td>
<td>$\left( 1 - \beta_1\right) \sum_{i=1}^t \left( \beta_1^{t-i} g_i \right)$</td>
<td>$\left( 1 - \beta_2\right) \sum_{i=1}^t \left( \beta_2^{t-i}\mathrm{diag}(g_i)^2 \right)$</td>
</tr>
</tbody>
</table>

</div>


<p>In the above frame work, we apply the projection on the parameters in the last step. Here we define the projection as</p>

<div class="overflow">
\begin{align}
    \Pi_{\mathcal{F},~\mathbf{A}} (\mathbf{x}) = \arg \min\limits_{\mathbf{z} \in \mathcal{F}} \lVert \mathbf{A}^{\frac{1}{2}} \left( \mathbf{z} - \mathbf{x} \right) \rVert_2^2.
\end{align}
</div>

<p>where $\mathcal{F}$ is the feasible domain and $\mathbf{A}$ is a diagonal matrix. The projection is required especially when we apply constraints on the optimized parameters. This step could ensure that the parameter would not get out of the feasible domain. In Keras, such a technique has been realized. Users who are interested in the implementation could review <a href="https://keras.io/constraints/">the documentation</a>.</p>

<h2 id="non-convergence-of-adam">Non-convergence of Adam</h2>

<h3 id="start-from-a-strong-assumption">Start from a strong assumption</h3>

<p>The author has proved that Adam would not get converged on such a loss function:</p>

<div class="overflow">
\begin{equation}
    f_t (x) = \left\{
    \begin{array}{l    r}
    C x, & \mod (t,~3)=1 \\[5pt]
    -x,  & \mathrm{otherwise}
    \end{array}
    \right.
\end{equation}
</div>

<p>where $C&gt;2$ and the feasible domain is $x \in [-1,~1]$. Note that this loss function is linear and convex.</p>

<p>To learn how it happens, first we need to confirm the <strong>optimal regret</strong>. Regret function measures the distance between the curren losses and optimal losses in the whole learning process. It is defined as</p>

<div class="overflow">
\begin{align}
    R_{T} = \sum_{t=1}^T f_t (\mathbf{x}_t) - \min\limits_{\mathbf{x} \in \mathcal{F}} \sum_{t=1}^T f_t (\mathbf{x}).
\end{align}
</div>

<p>To find the optimal regret, we only need to locate the minimal point (optimal solution) for the loss function $f_t$. Consider the stochastic function <a href="#mjx-eqn-2">$(2)$</a>, we could find that the expectation of this function is $\mathbb{E}_{t} [f_t (x)] = (\frac{1}{3} C - 1) x$. Since $C&gt;2$, we would find that when $x=-1$, this function has minimal expectation $1 - \frac{1}{3} C$.</p>

<p>Now let us consider the gradient of $f_t (x)$.</p>

<div class="overflow">
\begin{equation}
    \nabla f_t (x) = \left\{
    \begin{array}{l    r}
    C, & \mod (t,~3)=1 \\[5pt]
    -1,  & \mathrm{otherwise}
    \end{array}
    \right.
\end{equation}
</div>

<p>For the Adam, we define the hyperparameters as</p>


<div class="mintable">

  <table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>

<tbody>
<tr>
<td>$\beta_1$</td>
<td>0</td>
</tr>

<tr>
<td>$\beta_2$</td>
<td>$\dfrac{1}{1 + C^2}$</td>
</tr>

<tr>
<td>$\alpha_t$</td>
<td>$\frac{\alpha}{\sqrt{t}}$</td>
</tr>

<tr>
<td>$\mathcal{F}$</td>
<td>$[-1,~1]$</td>
</tr>
</tbody>
</table>

</div>


<p>The value of $\beta_2$ is a strong assumption. We would derive how to get this assumption later. Although in most cases, such an assumption may not hold, we could still start from it to begin our derivation.</p>

<p>Assume that $\mathbf{x}_{3t+1} = 1$, then we have $\mathbf{g}_{3t+1} = C$, and</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        \hat{\mathbf{x}}_{3t+2} &= \mathbf{x}_{3t+1} - \frac{\alpha}{\sqrt{3t+1}} \frac{1}{\sqrt{\beta_2 \mathbf{v}_{3t+1} + (1 - \beta_2) \mathbf{g}^2_{3t+1} }} \mathbf{g}_{3t+1} \\
        &= \mathbf{x}_{3t+1} - \frac{\alpha}{\sqrt{3t+1}} \frac{1}{\sqrt{\beta_2 \mathbf{v}_{3t+1} + (1 - \beta_2) C^2 }} C := \mathbf{x}_{3t+1} - T_1,
    \end{aligned}
\end{equation}
</div>

<p>where we define</p>

<div class="overflow">
\begin{align}
    T_1 := \frac{\alpha}{\sqrt{3t+1}} \frac{1}{\sqrt{\beta_2 \mathbf{v}_{3t+1} + (1 - \beta_2) C^2 }} C \leqslant \frac{\alpha}{\sqrt{(3t+1)(1 - \beta_2) }}.
\end{align}
</div>

<p>If we assume that the initial step size $\alpha$ is small enough, i.e. $\alpha &lt; \sqrt{1 - \beta_2}$, then we have $T_1 &lt; 1$, which means $0 &lt; \hat{\mathbf{x}}_{3t+2} &lt; 1$.</p>

<p>Note that in the last step we need to apply $\mathbf{x}_{3t+2} = \Pi_{\mathcal{F},~\mathbf{V}} (\hat{\mathbf{x}}_{3t+2})$. Since $\hat{\mathbf{x}}_{3t+2} \in (0,~1)$, we know that $\hat{\mathbf{x}}_{3t+2}$ is in feasible domain, and then we have $\mathbf{x}_{3t+2} \in (0,~1)$.</p>

<p>Then, consider $3t+2$ and $3t+3$, in the two steps, $\mathbf{g}_{3t+i} = -1$, then we have</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        \hat{\mathbf{x}}_{3t+3} &= \mathbf{x}_{3t+2} + \frac{\alpha}{\sqrt{3t+2}} \frac{1}{\sqrt{\beta_2 \mathbf{v}_{3t+2} + (1 - \beta_2) }} \\
        \hat{\mathbf{x}}_{3t+4} &= \mathbf{x}_{3t+3} + \frac{\alpha}{\sqrt{3t+3}} \frac{1}{\sqrt{\beta_2 \mathbf{v}_{3t+3} + (1 - \beta_2) }}.
    \end{aligned}
\end{equation}
</div>

<p>Since the updates are positive, from $\mathbf{x}_{3t+2} \in (0,~1)$ we could also infer that $\mathbf{x}_{3t+3},~\mathbf{x}_{3t+4} \in (0,~1]$.</p>

<p>Let us assume that $\hat{\mathbf{x}}_{3t+3} &gt; 1$, according to the projection, we have $\mathbf{x}_{3t+3} = 1$. because $\hat{\mathbf{x}}_{3t+4} &gt; \mathbf{x}_{3t+3}$, there would be $\mathbf{x}_{3t+4} = 1$.</p>

<p>The other case is $\mathbf{x}_{3t+3} = \hat{\mathbf{x}}_{3t+3} \in (0,~1)$. In this case, we need to combine <a href="#mjx-eqn-5">$(5)$</a> and <a href="#mjx-eqn-7">$(7)$</a> to determine the bound of $\mathbf{x}_{3t+4}$. First, we have</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        \hat{\mathbf{x}}_{3t+4} &= \mathbf{x}_{3t+1} - T_1 + \frac{\alpha}{\sqrt{3t+2}} \frac{1}{\sqrt{\beta_2 \mathbf{v}_{3t+2} + (1 - \beta_2) }} + \frac{\alpha}{\sqrt{3t+3}} \frac{1}{\sqrt{\beta_2 \mathbf{v}_{3t+3} + (1 - \beta_2) }} \\
        &:= \mathbf{x}_{3t+1} - T_1 + T_2.
    \end{aligned}
\end{equation}
</div>

<p>Like what we have done in <a href="#mjx-eqn-6">$(6)$</a>, we could define $T_2$ and find its bound. To determine the lower bound of $T_2$, we need to find the upper bound of the second order momemtum $\mathbf{v}_{3t+i}$. Since $|\mathbf{g}_{3t+i}| \leqslant C$, we have $\mathbf{v}_{3t+i} \leqslant C^2$, then</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        T_2 &:= \frac{\alpha}{\sqrt{3t+2}} \frac{1}{\sqrt{\beta_2 \mathbf{v}_{3t+2} + (1 - \beta_2) }} + \frac{\alpha}{\sqrt{3t+3}} \frac{1}{\sqrt{\beta_2 \mathbf{v}_{3t+3} + (1 - \beta_2) }} \\
        &\geqslant  \frac{\alpha}{\sqrt{3t+2}} \frac{1}{\sqrt{\beta_2 C^2 + (1 - \beta_2) }} + \frac{\alpha}{\sqrt{3t+3}} \frac{1}{\sqrt{\beta_2 C^2 + (1 - \beta_2) }} \\
        &= \frac{\alpha}{\sqrt{\beta_2 C^2 + (1 - \beta_2) }} \left( \frac{1}{\sqrt{3t+2}} + \frac{1}{\sqrt{3t+3}} \right) \\
        &\geqslant \frac{\alpha}{\sqrt{\beta_2 C^2 + (1 - \beta_2) }} \left( \frac{1}{\sqrt{2(3t+1)}} + \frac{1}{\sqrt{2(3t+1)}} \right) \\
        &= \frac{\sqrt{2}\alpha}{\sqrt{\beta_2 C^2 + (1 - \beta_2) }} \frac{1}{\sqrt{3t+1}}.
    \end{aligned}
\end{equation}
</div>

<p>To make $T_2 \geqslant T_1$, we need to let the following inequality hold:</p>

<div class="overflow">
\begin{align}
    T_2 \geqslant \frac{\sqrt{2}\alpha}{\sqrt{\beta_2 C^2 + (1 - \beta_2) }} \frac{1}{\sqrt{3t+1}} \geqslant \frac{\alpha}{\sqrt{(3t+1)(1 - \beta_2) }} \geqslant T_1.
\end{align}
</div>

<p>From <a href="#mjx-eqn-10">$(10)$</a>, we would get the condition $\beta_2 \leqslant \dfrac{1}{1 + C^2}$ easily. So when we let $\beta_2 0 \dfrac{1}{1 + C^2}$, we have $T_2 \geqslant T_1$, which means $\hat{\mathbf{x}}_{3t+4} \geqslant \mathbf{x}_{3t+1} = 1$.</p>

<p>Therefore, in any case, we would find that $\hat{\mathbf{x}}_{3t+4} \geqslant 1$, i.e. we could derive $\mathbf{x}_{3t+4} = 1$ when we start from $\mathbf{x}_{3t+1} = 1$. In this case, we know that for any $t$, there would be $f_{3t+1}(\mathbf{x}_{3t+1}) = C$. Since $f_{3t+1}(\mathbf{x}) \geqslant f_{3t+1}(-1) = -C$, $f_{3t+2}(\mathbf{\mathbf{x}_{3t+2}}),~f_{3t+3}(\mathbf{\mathbf{x}_{3t+3}}) \geqslant -1$ and $f_{3t+2}(\mathbf{-1}),~f_{3t+3}(\mathbf{-1}) = -1$, we could calculate the regret during the 3 steps:</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        R_{t,3} &= f_{3t+1}(\mathbf{x}_{3t+1}) + f_{3t+1}(\mathbf{x}_{3t+2}) + f_{3t+1}(\mathbf{x}_{3t+3}) + C - 1 - 1 \\
        &= 2C - 2 + f_{3t+1}(\mathbf{x}_{3t+2}) + f_{3t+1}(\mathbf{x}_{3t+3}) \geqslant 2C - 4,
    \end{aligned}
\end{equation}
</div>

<p>which means the regret could be represented as $R_T = \frac{R_{t,3}}{3} T \geqslant \frac{2C - 4}{3} T$, hence we know $\frac{R_T}{T} \nrightarrow 0$, the algorithm could not get converged.</p>

<p>This derivation is based on a specially designed loss function and three assumptions:</p>

<ul>
<li>The initial condition is $\mathbf{x}_{3t+1} = 1$;</li>
<li>The problem is constrained in a feasible domain $\mathcal{F}=[-1,~1]$;</li>
<li>The hyperparameter has a bound $\beta_2 \leqslant \dfrac{1}{1 + C^2}$.</li>
</ul>

<p>The third assumption may not hold for most cases. According to the suggestions from Kingma, we should define a large $\beta_2$ to improve the quality of the solution. Hence, to prove that Adam could not get converged, we still need to generalize this proof.</p>

<h3 id="generalize-the-proof-in-online-cases">Generalize the proof in online cases</h3>

<p>First, we generalize the loss function. For any $t$,</p>

<div class="overflow">
\begin{equation}
    f_t (x) = \left\{
    \begin{array}{l    r}
    C x, & \mod (t,~C)=1 \\[5pt]
    -x,  & \mathrm{otherwise}
    \end{array}
    \right.
\end{equation}
</div>

<p>where $C&gt;0$ and $\mod (C,~2)=0$. The feasible domain is still $x \in \mathcal{F} := [-1,~1]$. This loss function is linear and convex.</p>

<p>Similarly, the minimal point of this linear function is $f_t (-1)$, which means $\sum_{i=t}^{t+C-1} f_t (-1) = - C + ( C - 1 ) = - 1$.</p>

<p>And we still have the gradient</p>

<div class="overflow">
\begin{equation}
    \nabla f_t (x) = \left\{
    \begin{array}{l    r}
    C, & \mod (t,~C)=1 \\[5pt]
    -1,  & \mathrm{otherwise}
    \end{array}
    \right.
\end{equation}
</div>

<p>In the following part, we would try to prove that if $C$ is large enough, i.e the following conditions hold, we would prove that the algorithm could not get converged.</p>


<div class="mintable">

  <table>
<thead>
<tr>
<th>No.</th>
<th>Condition</th>
</tr>
</thead>

<tbody>
<tr>
<td><span id='cd-1'>1</span></td>
<td>$(1 - \beta_1) \beta_1^{C-1}C \leqslant 1 - \beta_1^{C-1}$</td>
</tr>

<tr>
<td><span id='cd-2'>2</span></td>
<td>$\beta_2^{(C-2)/2}C^2 \leqslant 1$</td>
</tr>

<tr>
<td><span id='cd-3'>3</span></td>
<td>$\dfrac{3(1 - \beta_1)}{2\sqrt{1 - \beta_2}} \left(1 + \dfrac{\gamma (1 - \gamma^{C-1})}{1 - \gamma} \right) + \dfrac{\beta_1^{C/2-1}}{1 - \beta_1} &lt; \dfrac{C}{3}$</td>
</tr>
</tbody>
</table>

</div>


<p>Assume that $\mod(t,C)=0$. The basic idea of the derivation is inferring $\mathbf{x}_{t+C} = 1$ from $\mathbf{x}_{t} = 1$. In the generalized case, we only assume that $\beta_1 \leqslant \sqrt{\beta_2}$. We start with confirming the bound of momentum $\mathbf{m}$ by <a href="#cd-1">Condition 1</a>:</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        \mathbf{m}_{t+C} &= - (1 - \beta_1) - (1 - \beta_1) \beta_1 - \cdots - (1 - \beta_1)\beta_1^{C-1}C + \beta_1^C \mathbf{m}_{t} \\
        &= - (1 - \beta_1^{C-1}) + (1 - \beta_1) \beta_1^{C-1}C + \beta_1^C \mathbf{m}_{t}.
    \end{aligned}
\end{equation}
</div>

<p>If <a href="#cd-1">Condition 1</a> holds, and let $\mathbf{m}_{t} \leqslant 0$, we could get $\mathbf{m}_{t+C} \leqslant 0$ from <a href="#mjx-eqn-14">$(14)$</a>.</p>

<p>Then we need to set a time point $T&rsquo;$ to ensure that for any large enough $t \leqslant T&rsquo;$, we would have $t + C \leqslant \tau^t t$, where $\tau \leqslant \frac{3}{2}$. Such a condition implies that we only talk about the case when $T \rightarrow \infty$.</p>

<p>After that, we need to specify the range of the second order momentum. Consider that</p>

<div class="overflow">
\begin{align}
    \mathbf{v}_{t+i-1} = (1 - \beta_2) \sum\limits_{j=1}^{t+i-1} \beta_2^{t+i-1-j} \mathbf{g}_j^2.
\end{align}
</div>

<p>If we only preserve the term $j=t+1$, we would have</p>

<div class="overflow">
\begin{align}
    \mathbf{v}_{t+i-1} \geqslant (1 - \beta_2) \beta_2^{i-2} C^2.
\end{align}
</div>

<p>Assume that $t \geqslant kC$, for $i&rsquo; \leqslant i \leqslant C$,</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        \mathbf{v}_{t+i-1} &\leqslant (1 - \beta_2) \left[ \sum\limits_{h=1}^k \beta_2^{t+i-1-hC} C^2 + \sum\limits_{j=1}^{t+i-1} \beta_2^{t+i-1-j} \right] \\
        &\leqslant (1 - \beta_2) \left[ \beta_2^{i'-1}C^2 \sum\limits_{h=0}^{k-1} \beta_2^{hC} + \frac{1}{1 - \beta_2} \right] \\
        &\leqslant (1 - \beta_2) \left[ \frac{\beta_2^{i'-1} C^2}{1 - \beta_2^C} + \frac{1}{1 - \beta_2} \right] \leqslant 2 .
    \end{aligned}
\end{equation}
</div>

<p>Why the last inequality holds? This is because of <a href="#cd-2">Condition 2</a>. Due to the condition, we know that we could find a $i&rsquo;$ that $\beta_2^{i&rsquo;-1}C^2 \leqslant 1$. So</p>

<div class="overflow">
\begin{equation}
    \begin{aligned}
        \beta_2^{i'-1} C^2 - \beta_2^{i'} C^2 &\leqslant 1 - \beta_2^{C} C^2 \leqslant 1 - \beta_2^{C}. \\
        (1 - \beta_2) \left( \beta_2^{i'-1} C^2 \right) &\leqslant 1 - \beta_2^C. \\
        (1 - \beta_2) \frac{\beta_2^{i'-1} C^2}{1 - \beta_2^C} &\leqslant 1.
    \end{aligned}
\end{equation}
</div>

<div class="box" style="color:#FB8; padding-bottom:40px">
  <div style="float: left;"> 
    <p><i class="fa fa-exclamation-triangle" aria-hidden="true" style="font-size: 50px"></i></p>
  </div>
  <div style="margin-left: 60px; font-style:normal;">
    <p>This article is still being produced. Please wait for several days to see the full edition.</p>
  </div>
</div>

                        </div>
                    </section>
            <!-- Disqus Inject -->
                
                  <section>
    <div class="inner" id="disqus_thread"></div>
    <script type="text/javascript">

    (function() {
          
          
          if (window.location.hostname == "localhost")
                return;

          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          var disqus_shortname = 'rosenkreutz-studio';
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <div class="inner"><a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></div>
</section>
                
            </div>
            
        <!-- Footer -->
            
                <!-- Footer -->
    <footer id="footer">
        <div class="inner">
            <ul class="icons">
                
                    <li><a href="mailto:cainmagi@gmail.com" class="icon alt fa-envelope" target="_blank"><span class="label">Email</span></a></li>
                
                    <li><a href="https://weibo.com/u/5885093621" class="icon alt fa-weibo" target="_blank"><span class="label">Weibo</span></a></li>
                
                    <li><a href="https://github.com/cainmagi" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a></li>
                
                    <li><a href="https://steamcommunity.com/id/cainmagi" class="icon alt fa-steam" target="_blank"><span class="label">Steam</span></a></li>
                
                    <li><a href="https://www.youtube.com/channel/UCzqpNK5qFMy5_cI1i0Z1nQw" class="icon alt fa-youtube-play" target="_blank"><span class="label">Youtube</span></a></li>
                
                    <li><a href="https://music.163.com/#/user/home?id=276304206" class="icon alt fa-music" target="_blank"><span class="label">Netease Music</span></a></li>
                
            </ul>
            <ul class="copyright">
                <li>&copy; Well-logging laboratory, Department of Electrical and Computer Engineering, University of Houston</li>
                
            </ul>
        </div>
    </footer>

            
        </div>

    <!-- Scripts -->
        <!-- Scripts -->
    <!-- jQuery -->
    <script src="https://cainmagi.github.io/js/jquery.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.scrolly.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.scrollex.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.elevatezoom.js" type="text/javascript"></script>
    <script src="https://cainmagi.github.io/js/jquery.images.js"></script>
    <script src="https://cainmagi.github.io/js/skel.min.js"></script>
    <script src="https://cainmagi.github.io/js/util.js"></script>
    <script type="text/javascript" src="https://cainmagi.github.io/js/tooltipster.bundle.min.js"></script>

    

    <!-- Main JS -->
    <script src="https://cainmagi.github.io/js/main.js"></script>
    <script src="https://cainmagi.github.io/js/extensions.js"></script>
    
    
    <script src="https://cainmagi.github.io/js/title.js"></script>
    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-119875813-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    
    
    <script src="https://cainmagi.github.io/js/highlight.pack.js"></script>
    <link rel="stylesheet" href="https://cainmagi.github.io/css/vs2015adp.css">
    <script>hljs.initHighlightingOnLoad();</script>
    
    <script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
          extensions: ["AMSmath.js", "AMSsymbols.js", "boldsymbol.js", "color.js"]
      }
    }
  });

  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
    
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

    

    

    </body>
</html>
