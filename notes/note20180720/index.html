<!DOCTYPE HTML>
<html>
    <!-- Header -->
    <head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<meta name="description" content="A Ph.D student in University of Houston (UH). Interested area includes: machine learning, programming and religion.">
	<meta name="author" content="Yuchen Jin">
	
	<meta name="generator" content="Hugo 0.54.0" />
	<title>Notes on Jul. 20, 2018 &middot; Rosenkreutz Studio</title>
	<!-- Stylesheets -->
	
	<link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.3.1/semantic.min.css" rel="stylesheet" type="text/css">
	<link rel="stylesheet" type="text/css" href="https://cainmagi.github.io/css/tooltipster.bundle.min.css" />
	<link rel="stylesheet" type="text/css" href="https://cainmagi.github.io/css/tooltipster-sideTip-borderless.min.css" />
	<link rel="stylesheet" href="https://cainmagi.github.io/css/main.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/title.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/extensions.css"/>
	<link rel="stylesheet" href="https://cainmagi.github.io/css/jq-images.css"/>
	
	

	

	<!-- Custom Fonts -->
	<link href="https://cainmagi.github.io/css/font-awesome.min.css" rel="stylesheet" type="text/css">
	<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

	
	<link rel="shortcut icon" type="image/x-icon" href="https://cainmagi.github.io/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://cainmagi.github.io/favicon.ico">
	

	<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	<!--[if lt IE 9]>
	<script src="js/ie/html5shiv.js"></script>
	<script src="js/ie/html5shiv.jsrespond.min.js"></script>
	<![endif]-->
</head>

    <body>

    <!-- Wrapper -->
    <div id="wrapper">

            <!-- Header -->
    <header id="header" class="alt">
        <a href="https://cainmagi.github.io/" class="logo"><strong>CainMagi</strong> <span>University of Houston</span></a>
        <nav>
            <a href="#menu">Menu</a>
        </nav>
    </header>

<!-- Menu -->
    <nav id="menu">
        <ul class="links">
            
                <li><a href="https://cainmagi.github.io/">Home</a></li>
            
                <li><a href="https://cainmagi.github.io/about">About</a></li>
            
                <li><a href="https://cainmagi.github.io/notes">Notes</a></li>
            
                <li><a href="https://cainmagi.github.io/researches">Researches</a></li>
            
                <li><a href="https://cainmagi.github.io/projects">Projects</a></li>
            
                <li><a href="https://cainmagi.github.io/playground">Playground</a></li>
            

        </ul>
        <ul class="actions vertical">
            
                <li><a href="http://welllogging.egr.uh.edu/" class="button special fit">Laboratory Page</a></li>
            
            
        </ul>
    </nav>

        <!-- Main -->
            <div id="main" class="alt">

                
                    <section id="one">
                        <div class="inner">
                            <header id="pagetitle" class="major">
                                <h1 id='main_title'>Notes on Jul. 20, 2018</h1>
                                <table class="sub-title">
                                    <tbody>
                                        <tr>
                                            <th>Date:</th>
                                            <td>Jul 20, 2018</td>
                                        </tr> 
                                        <tr>
                                            <th>Last Updated:</th>
                                            <td>Aug 10, 2018</td>
                                        </tr>
                                        <tr>
                                            <th>Categories:</th>
                                            <td><section class="dream-tags">
                                                    <a class="ui label categ" href="/categories/notes" title="Notes">Notes</a>
                                                    
                                                    <a class="ui label categ" href="/categories/drafts" title="Drafts">Drafts</a>
                                                    
                                                
                                            </section></td>
                                        </tr>
                                        <tr>
                                            <th>Tags:</th>
                                            <td><section class="dream-tags">
                                                    <a class="ui label" href="/tags/research" title="research">research</a>
                                                    
                                                    <a class="ui label" href="/tags/deep-learning" title="deep-learning">deep-learning</a>
                                                    
                                                    <a class="ui label" href="/tags/inverse-problem" title="inverse-problem">inverse-problem</a>
                                                    
                                                    <a class="ui label" href="/tags/seismic-processing" title="seismic-processing">seismic-processing</a>
                                                    
                                                
                                            </section></td>
                                        </tr>
                                    </tbody>
                                </table>
                                
                                <span class="image main"><img src="/img/notes/draft.jpg" alt="" /></span>
                                
                            </header>
                            
                            <hr/>
                            <h1 id="contents">Contents</h1>
                            <p><nav id="TableOfContents">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#text">Text</a>
<ul>
<li><a href="#fully-relevant">Fully relevant</a>
<ul>
<li><a href="#three-dimensional-defect-inversion-from-magnetic-flux-leakage-signals-using-iterative-neural-network">Three-dimensional defect inversion from magnetic flux leakage signals using iterative neural network</a></li>
<li><a href="#geoacoustic-model-inversion-with-artificial-neural-networks">Geoacoustic model inversion with artificial neural networks</a></li>
<li><a href="#electromagnetic-nde-signal-inversion-by-function-approximation-neural-networks">Electromagnetic NDE signal inversion by function-approximation neural networks</a></li>
<li><a href="#deep-learning-for-the-design-of-nano-photonic-structures">Deep Learning for the Design of Nano-photonic Structures</a></li>
<li><a href="#comparison-of-detection-abilities-between-fluxgate-and-gmr-magnetometer-in-inverse-ect-of-deep-lying-cracks">Comparison of Detection Abilities Between Fluxgate and GMR Magnetometer in Inverse ECT of Deep Lying Cracks</a></li>
<li><a href="#a-neural-network-approach-for-the-inversion-of-multi-scale-roughness-parameters-and-soil-moisture">A Neural Network Approach for the Inversion of Multi-Scale Roughness Parameters and Soil Moisture</a></li>
</ul></li>
<li><a href="#topic-relevant">Topic relevant</a>
<ul>
<li><a href="#scaling-cnns-for-high-resolution-volumetric-reconstruction-from-a-single-image">Scaling CNNs for High Resolution Volumetric Reconstruction from a Single Image</a></li>
<li><a href="#tradeoffs-between-convergence-speed-and-reconstruction-accuracy-in-inverse-problems">Tradeoffs Between Convergence Speed and Reconstruction Accuracy in Inverse Problems</a></li>
<li><a href="#onsager-corrected-deep-learning-for-sparse-linear-inverse-problems">Onsager-corrected deep learning for sparse linear inverse problems</a></li>
<li><a href="#dagan-deep-de-aliasing-generative-adversarial-networks-for-fast-compressed-sensing-mri-reconstruction">DAGAN Deep De-Aliasing Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction</a></li>
</ul></li>
<li><a href="#referable">Referable</a>
<ul>
<li><a href="#using-deep-neural-networks-for-inverse-problems-in-imaging">Using Deep Neural Networks for Inverse Problems in Imaging</a></li>
<li><a href="#estimate-articulatory-mri-series-from-acoustic-signal-using-deep-architecture">Estimate articulatory MRI series from acoustic signal using deep architecture</a></li>
</ul></li>
</ul></li>
</ul>
</nav></p>
                            
                            <hr/>
                            

<h1 id="introduction">Introduction</h1>

<p>Here we would like to discuss about the some papers using deep learning methods to enhance the performance of the traditional inverse problems. The application should be limited in analyzing the electromagnetic wave or acoustic signal. Some works just make the application of existing methods, some works propose fundamental theory and some works give us an inspiration of available architectures. According to the relevance, we would like to divide them into 3 groups.</p>

<ul>
<li><a href="#fully-relevant"><strong>Fully relevant</strong></a>: Highly relevant to our topic and for the same application, but may be out of date.</li>
<li><a href="#topic-relevant"><strong>Topic relevant</strong></a>: Highly relevant to our topic, but the application is different.</li>
<li><a href="#referable"><strong>Referable</strong></a>: Not very relevant to out topic, but providing valuable information.</li>
</ul>

<h1 id="text">Text</h1>

<h2 id="fully-relevant">Fully relevant</h2>

<h3 id="three-dimensional-defect-inversion-from-magnetic-flux-leakage-signals-using-iterative-neural-network">Three-dimensional defect inversion from magnetic flux leakage signals using iterative neural network</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: Three-dimensional defect inversion from magnetic flux leakage signals using iterative neural network</li>
        <li><b>Author</b>: Junjie Chen, Songling Huang, and Wei Zhao</li>
        <li><b>Year</b>: 2015</li>
        <li><b>Theory level</b>: application</li>
        <li><b>Theory type</b>: network architecture: ANN</li>
        <li><b>Used data</b>: The magnetic flux leakage (MFL) signal</li>
        <li><b>Predicted data</b>: The shape of a 3D defect profile</li>
        <li><b>Source</b>: IET Science, Measurement & Technology</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/7138677/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>In this paper, the author uses a 3 layer radial basis function (RBF) ANN to simulate the forward model of the EM response from a defect profile. Different from the normal ANN, the layer of RBF ANN is defined by two parts: denote $\mathbf{x}$ as the input, $\mathbf{y}$ as the output, the $j^{\mathrm{th}}$ element of the hidden layer $\mathbf{h}$ could be defined as</p>
        <div class="overflow">
        \begin{align}
            h_j = \exp \left( -\frac{\lVert \mathbf{x} - c_j \rVert^2_2}{\sigma_j} \right).
        \end{align}
        </div>
        <p>And the output layer is</p>
        <div class="overflow">
        \begin{align}
            \mathbf{y}^T = \mathbf{W} \mathbf{h}^T.
        \end{align}
        </div>
        <p>Thus we know that RBF is an adaptation for the hidden layer. The optimization of this work could be figured as below:</p>
        <p><img src="JJChen2015.jpg"/></p>
    </div>
  </div>
</div>

<h3 id="geoacoustic-model-inversion-with-artificial-neural-networks">Geoacoustic model inversion with artificial neural networks</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: Geoacoustic model inversion with artificial neural networks</li>
        <li><b>Author</b>: J. Benson, N.R. Chapman, and A. Antoniou</li>
        <li><b>Year</b>: 1998</li>
        <li><b>Theory level</b>: application</li>
        <li><b>Theory type</b>: network architecture: ANN</li>
        <li><b>Used data</b>: acoustic signal</li>
        <li><b>Predicted data</b>: geophysical parameters</li>
        <li><b>Source</b>: IEEE Symposium on Advances in Digital Filtering and Signal Processing. Symposium Proceedings</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/685708/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>In this paper, the author use a vertical line of receivers to get the reflected acoustic signal in the ocean. By analyzing the received signal, we try to get some parameters like the boundary between the geophysical surfaces, the density of the material, the signal speed and some other results. The experiments are shown in the following figure.</p>
        <p><img src="Benson1998.png"/></p>
        <p>Both the traditional MLP (ANN) and RBF ANN are used to simulate the inversion of the model. The ANN contains 104 hidden nodes while RBF contains 1200 hidden nodes. The results show that RBF could reach a better result. However, the MSE is only improved marginally by RBF (8.88->8.35).</p>
    </div>
  </div>
</div>

<h3 id="electromagnetic-nde-signal-inversion-by-function-approximation-neural-networks">Electromagnetic NDE signal inversion by function-approximation neural networks</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: Electromagnetic NDE signal inversion by function-approximation neural networks</li>
        <li><b>Author</b>: P. Ramuhalli, L. Udpa, and S.S. Udpa</li>
        <li><b>Year</b>: 2002</li>
        <li><b>Theory level</b>: application</li>
        <li><b>Theory type</b>: network architecture: ANN</li>
        <li><b>Used data</b>: The magnetic flux leakage (MFL) signal</li>
        <li><b>Predicted data</b>: The shape of an 1D defect profile</li>
        <li><b>Source</b>: IEEE Transactions on Magnetics</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/1158952/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>The idea of this work is almost exactly the same as <a href="#three-dimensional-defect-inversion-from-magnetic-flux-leakage-signals-using-iterative-neural-network">this</a> one. It seems that the paper in 2015 is just a reproduction of this work (in 2002) in the 3D data field.</p>
        <p>Note that in this paper, the author also proposes another model, Wavelet-Basis Function Neural Networks(WBF ANN), which could be figured as</p>
        <p style="text-align:center"><img src="Ramuhalli2002.jpg"/></p>
        <p>The layer in WBF ANN could be defined as</p>
        <div class="overflow">
        \begin{align}
            y_n = \sum\limits_{k=1}^{K_L} w_{nk} \phi_k (\mathbf{x},~\mathbf{c}_k) + \sum\limits_{l=1}^{L} \sum\limits_{k=1}^{K_n} w_{nlk} \psi_{lk} (\mathbf{x},~\mathbf{c}_{lk}),
        \end{align}
        </div>
        <p>where we use $k$ to represent the output value with different offsets of the same scaling/wavelet function, $l$ represent the resolution stage (where $L$ is the the coarsest one) and $n$ to represent the $n^\mathrm{th}$ output value. $\phi$ is the scaling function and the $\psi$ is the wavelet function. The results show that compared to MLP ANN and RBF ANN, the WBF ANN has the best performance.</p>
    </div>
  </div>
</div>

<h3 id="deep-learning-for-the-design-of-nano-photonic-structures">Deep Learning for the Design of Nano-photonic Structures</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: Deep Learning for the Design of Nano-photonic Structures</li>
        <li><b>Author</b>: Itzik Malkiel, Michael Mrejen, Achiya Nagler, Uri Arieli, Lior Wolf, and Haim Suchowski</li>
        <li><b>Year</b>: 2018</li>
        <li><b>Theory level</b>: application</li>
        <li><b>Theory type</b>: network architecture: Deep ANN</li>
        <li><b>Used data</b>: spectrum signal</li>
        <li><b>Predicted data</b>: nano-photonic structure parameters</li>
        <li><b>Source</b>: IEEE International Conference on Computational Photography</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/8368462/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>The issue of this work is based on the inspection of optical response transmitted through a nano-photonic material. The author assume that the basic structure of this kind of material could be described by 5 units with a sub-wavelength geometry. Here is the network structure.</p>
        <p><img src="Malkiel2018.png"/></p>
        <p>(A) shows that the network accept the transmitted spectrum in both directions and the material's geometries. Then it would predict the nano-structure's geometries $\mathbf{g}$. The output geometries would also be used to reconstruct the spectrums $\mathbf{s}_x$, $\mathbf{s}_y$ (also in both directions) so that it could be updated by the reconstruction gradients and the supervised gradients. The loss function could be written as:</p>
        <div class="overflow">
        \begin{align}
            \mathcal{L} = \mathrm{MSE}(\mathbf{g} ,~ \mathbf{g}_0) + \mathrm{MSE}(\mathbf{s}_x ,~ \mathbf{s}_{x0}) + \mathrm{MSE}(\mathbf{s}_y ,~ \mathbf{s}_{y0}),
        \end{align}
        </div>
        <p>where we use $\mathbf{g}_0,~\mathbf{s}_{x0}$ and $\mathbf{s}_{y0}$ to represent the ground truth.</p>
        <p>(B) and (C) are two examples of the testing results. In the southwest corner we show the true and predicted geometry. The curves are reconstructed spectrum, which is an estimation of the accuracy of the prediction.</p>
    </div>
  </div>
</div>

<h3 id="comparison-of-detection-abilities-between-fluxgate-and-gmr-magnetometer-in-inverse-ect-of-deep-lying-cracks">Comparison of Detection Abilities Between Fluxgate and GMR Magnetometer in Inverse ECT of Deep Lying Cracks</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: Comparison of Detection Abilities Between Fluxgate and GMR Magnetometer in Inverse ECT of Deep Lying Cracks</li>
        <li><b>Author</b>: Lukas Behun, Milan Smetana, and Klara Capova</li>
        <li><b>Year</b>: 2018</li>
        <li><b>Theory level</b>: application</li>
        <li><b>Theory type</b>: network architecture: ANN</li>
        <li><b>Used data</b>: The magnetic measurements</li>
        <li><b>Predicted data</b>: The shape parameters of defect</li>
        <li><b>Source</b>: ELEKTRO CONFERENCE PROCEEDINGS</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/8398332/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>In this work, the author use a coil to generate eddy currents on the surface of a sample. Such currents would stimulate a magnetic field, which would be captured by another coil. In practice, the receiver could be viewed as a magnetometer. By analyzing the response returned from the magnetometer, we could infer the parameters (width, length and depth) of the shape of the cracks on the surface.</p>
        <p style="text-align:center"><img src="Behun2018.jpg"/></p>
        <p>The collected data would be decomposed by Haar wavelet, then the changes of the gradient of electric scalar potential could be analyzed by the artificial neural network (ANN) which accepts the decomposed data and predict the results like segmentation. By checking the region that is tagged by "gradient change" we could find the shape of the cracks.</p>
    </div>
  </div>
</div>

<h3 id="a-neural-network-approach-for-the-inversion-of-multi-scale-roughness-parameters-and-soil-moisture">A Neural Network Approach for the Inversion of Multi-Scale Roughness Parameters and Soil Moisture</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: A Neural Network Approach for the Inversion of Multi-Scale Roughness Parameters and Soil Moisture</li>
        <li><b>Author</b>: L.B. Farah, I.R. Farah, R. Bennaceur, Z. Belhadj, and M.R. Boussema</li>
        <li><b>Year</b>: 2006</li>
        <li><b>Theory level</b>: application</li>
        <li><b>Theory type</b>: network architecture: ANN</li>
        <li><b>Used data</b>: The radar backscattering coefficients</li>
        <li><b>Predicted data</b>: The shape coefficients and conductivity</li>
        <li><b>Source</b>: International Conference on Information & Communication Technologies</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/1684404/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>This work is an inspection on the surface of a material (actually is the soil). The author build a mathematical wavelet model and use a random process as the parameters of the model to describe the shape of the 2D interface. The parameters of the process, i.e. $\gamma,~\nu$ are used to describe the stationary part of the self autocorrelation that model. Apart from this, $\varepsilon$ represents the conductivity of the soil. Each of these parameters vary from different directions ($x$ and $y$ directions).</p>
        <p>Then the radar backscattering coefficient is used to analyze this material. By using different incept angle, we could record different coefficients $\sigma(\theta)$ in two direction (horizontal and vertical). These values are feed into a 4-layer neural network so that it could predict the model parameters discussed before. The error rate of the prediction is about 8%.</p>
        <p style="text-align:center"><img src="Farah2006.png"/></p>
    </div>
  </div>
</div>

<h2 id="topic-relevant">Topic relevant</h2>

<h3 id="scaling-cnns-for-high-resolution-volumetric-reconstruction-from-a-single-image">Scaling CNNs for High Resolution Volumetric Reconstruction from a Single Image</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: Scaling CNNs for High Resolution Volumetric Reconstruction from a Single</li>
        <li><b>Author</b>: Adrian Johnston, Ravi Garg, Gustavo Carneiro, and Ian Reid</li>
        <li><b>Year</b>: 2017</li>
        <li><b>Theory level</b>: application</li>
        <li><b>Theory type</b>: network architecture: residual</li>
        <li><b>Used data</b>: RGB image</li>
        <li><b>Predicted data</b>: 3D reconstructed model</li>
        <li><b>Source</b>: 2017 IEEE International Conference on Computer Vision Workshops</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/8265323/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>In this work the authors use a real forward function (iDCT) to replace the decoder part of the auto-encoder. Note that because the forward function here is simple, it should be compatible to be built by basic tensor ops. Therefore, in this work we do not need to define the gradient back propagated from iDCT specifically. The encoder is a residual network and the defined decoder could reach a better performance than the deep learning decoder.</p>
        <p><img src="Johnston2017.png"/></p>
    </div>
  </div>
</div>

<h3 id="tradeoffs-between-convergence-speed-and-reconstruction-accuracy-in-inverse-problems">Tradeoffs Between Convergence Speed and Reconstruction Accuracy in Inverse Problems</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: Tradeoffs Between Convergence Speed and Reconstruction Accuracy in Inverse Problems</li>
        <li><b>Author</b>: Raja Giryes, Yonina C. Eldar, Alex M. Bronstein, and Guillermo Sapiro</li>
        <li><b>Year</b>: 2018</li>
        <li><b>Theory level</b>: fundamental</li>
        <li><b>Theory type</b>: pure optimization issue</li>
        <li><b>Used data</b>: Gray scale image (patched as vectors)</li>
        <li><b>Predicted data</b>: DCT transformed patches</li>
        <li><b>Source</b>: IEEE TRANSACTIONS ON SIGNAL PROCESSING</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/8253896/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
        <ul>
        <li><b>External Reference</b>: check this link to learn theory about projected gradient algorithm (PGD). Note that PGD is difficult to be realized well in tensorflow due to the slow convergent rate.</li>
        </ul>
        <p style="text-indent:2em"><a href="http://freemind.pluskid.org/machine-learning/projected-gradient-method-and-lasso/" class="button icon fa-file-text-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>The authors introduce three baseline methods including iterative shrinkage-thresholding algorithm (ISTA), learned ISTA (LISTA) and projected gradient algorithm (PGD). Then they propose a new method named inexact projected gradient descent algorithm (IPGD). The problem they need to solve is, assuming that we have known $\mathbf{M}$ and response $\mathbf{y}$, we need to get</p>
        <div class="overflow">
        \begin{align} \label{top:Giryes:1}
            \hat{\mathbf{x}} = \arg\min\limits_{\mathbf{x}} \lVert \mathbf{y} - \mathbf{M}\mathbf{x} \rVert^2_2 + \lambda f(\mathbf{x}),
        \end{align}
        </div>
        <p>where $f$ is a regularization function. The ISTA is based on proximal optimization on the primal gradient. The PGD is based on orthogonal projection on the primal gradient. The iPGD uses a tree structure to improve the efficiency of PGD. Note that the author also introduces a learning-based method, i.e. LISTA, which uses a neural network to learn two matrices $\mathbf{U},~\mathbf{A}$ and updates the arguments by</p>
        <div class="overflow">
        \begin{align}
            \mathbf{z}_{t+1} = \mathrm{prox}_{\lambda f}(\mathbf{A}\mathbf{y} + \mathbf{U}\mathbf{z}_t),
        \end{align}
        </div>
        <p>where $\mathbf{z}_t$ is the optimized arguments by $t$ iterations and $\mathrm{prox}$ is the proximal operator which could be referred <a href="../note20180605special/#basic-definition" title="Proximal operator">here</a>.</p>
    </div>
  </div>
</div>

<h3 id="onsager-corrected-deep-learning-for-sparse-linear-inverse-problems">Onsager-corrected deep learning for sparse linear inverse problems</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: Onsager-corrected deep learning for sparse linear inverse problems</li>
        <li><b>Author</b>: Mark Borgerding, and Philip Schniter</li>
        <li><b>Year</b>: 2016</li>
        <li><b>Theory level</b>: fundamental</li>
        <li><b>Theory type</b>: deep learning & optimization</li>
        <li><b>Used data</b>: randomly generated data</li>
        <li><b>Predicted data</b>: randomly generated data</li>
        <li><b>Source</b>: IEEE Global Conference on Signal and Information Processing</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/7905837/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
        <ul>
        <li><b>External Reference</b>: check this link to see the implementation of proximal Adgrad in tensorflow.</li>
        </ul>
        <p style="text-indent:2em"><a href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalAdagradOptimizer" class="button icon fa-file-text-o" style="text-indent:0em">Reference</a></p>
        <ul>
        <li><b>External Reference</b>: check this link to see the theory of learning ISTA (LISTA).</li>
        </ul>
        <p style="text-indent:2em"><a href="http://yann.lecun.com/exdb/publis/pdf/gregor-icml-10.pdf" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>The authors introduce four baseline methods including iterative shrinkage-thresholding algorithm (ISTA), fast ISTA (FISTA), learned ISTA (LISTA) and approximate message passing (AMP) algorithm. Then they propose a new method, learned AMP (LAMP). The idea is implementing the idea from LISTA to AMP. The problem is almost the same as what we discussed in $\eqref{top:Giryes:1}$</p>
        <p>The basic idea of AMP could be formulated as</p>
        <div class="overflow">
        \begin{equation}
            \left\{
            \begin{aligned}
                \mathbf{v}_t &= \mathbf{y} - \mathbf{M} \hat{\mathbf{x}}_t + b_t \mathbf{v}_{t-1}, \\[5pt]
                \hat{\mathbf{x}}_{t+1} &= \mathrm{prox}_{\lambda_t \ell_1} \left( \mathbf{x}_t + \mathbf{M}^{H} \mathbf{v}_t \right), \\[5pt]
                b_t &= \frac{1}{M} \lVert \hat{\mathbf{x}}_t \rVert_0, \\[5pt]
                \lambda_t &= \frac{\alpha}{\sqrt{M}} \lVert \hat{\mathbf{v}}_t \rVert_2.
            \end{aligned}
            \right.
        \end{equation}
        </div>
        <p>Like what we do in LISTA, migrating to LAMP needs us to make some above parameters adjustable. Thus we define $\mathbf{A}_t = \beta_t \mathbf{M}$ and $\mathbf{B}_t = \mathbf{M}^H \mathbf{C}_t$. Then what could adjust is $\{\alpha_t, \beta_t, \mathbf{C}_t\}$, where $t$ is the step number of AMP but here is the layer number of the network. The following figure show the basic architecture of one layer of the network. The experiments show that the layer of the network could be much fewer than the step number of the primal inversion.</p>
        <p><img src="Borgerding2016.png"/></p>
    </div>
  </div>
</div>

<h3 id="dagan-deep-de-aliasing-generative-adversarial-networks-for-fast-compressed-sensing-mri-reconstruction">DAGAN Deep De-Aliasing Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: DAGAN Deep De-Aliasing Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction</li>
        <li><b>Author</b>: Guang Yang, Simiao Yu, Hao Dong, Greg Slabaugh, Pier Luigi Dragotti, Xujiong Ye , Fangde Liu, Simon Arridge, Jennifer Keegan, Yike Guo, and David Firmin</li>
        <li><b>Year</b>: 2018</li>
        <li><b>Theory level</b>: application</li>
        <li><b>Theory type</b>: network architecture: GAN</li>
        <li><b>Used data</b>: compressed magnetic resonance imaging (CS-MRI) data</li>
        <li><b>Predicted data</b>: original MRI data</li>
        <li><b>Source</b>: IEEE Transactions on Medical Imaging</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/8233175/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
        <ul>
        <li><b>External Reference</b>: check this link to see the theory about conditional GAN.</li>
        </ul>
        <p style="text-indent:2em"><a href="https://arxiv.org/abs/1411.1784" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
        <ul>
        <li><b>External Reference</b>: check this link to see a similar theory applied in image super-resolution, which is a more famous work.</li>
        </ul>
        <p style="text-indent:2em"><a href="https://arxiv.org/abs/1609.04802" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>Denote the original image (reshaped as a vector with a length of $M$) as $\mathbf{x}$, its corresponding compressed image is $\mathbf{y}$ ($\mathbf{y}$ has a length of $N$ which fulfils $N \ll M$). Then when we only know $\mathbf{y}$, we could use a series of basis (as a matrix $\mathbf{F}_u$ with a size of $N \times M$) to recover $\mathbf{x}$ from $\mathbf{y}$ by</p>
        <div class="overflow">
        \begin{align}
            \hat{\mathbf{x}} = \arg\min\limits_{\mathbf{z}} \lVert \mathbf{z} - \mathbf{F}_u\mathbf{z} \rVert^2_2 + \lambda f(\mathbf{z}),
        \end{align}
        </div>
        <p>Where $f$ is a function about regularization. This is a conventional inversion method.</p>
        <p>To replace this process, the authors propose a deep network to predict $\mathbf{x}$. First, we use the transposed basis to get a initialized prediction $\mathbf{x}_u$:</p>
        <div class="overflow">
        \begin{align}
            \mathbf{x}_u = \mathbf{F}^H_u \mathbf{y}.
        \end{align}
        </div>
        <p>Then we use a generic network to get a residual $G(\mathbf{x})_u$. The desired result would be formulated as $\hat{\mathbf{x}} = \mathbf{x}_u + G(\mathbf{x}_u)$. To make sure the prediction could be precise and in detail, we use a loss function with 4 parts to train the network: (1) pixel-wise MSE loss, (2) spectral-element-wise MSE loss, (3) a VGG-output loss that is used to estimate the difference between the prediction and the ground truth, (4) discriminative loss produced by GAN. In special, (4) involves a discriminator which is used to tell the difference between the ground truth $\mathbf{x}_t$ and the predicted result $\hat{\mathbf{x}}$. This idea comes from conditional GAN that need us to let the discriminator could not tell the difference when giving a pair of ground truth and the prediction.</p>
        <p><img src="Yang2018.svg"/></p>
    </div>
  </div>
</div>

<h2 id="referable">Referable</h2>

<h3 id="using-deep-neural-networks-for-inverse-problems-in-imaging">Using Deep Neural Networks for Inverse Problems in Imaging</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: Using Deep Neural Networks for Inverse Problems in Imaging</li>
        <li><b>Author</b>: Alice Lucas, Michael Iliadis, Rafael Molina, and Aggelos K. Katsaggelos</li>
        <li><b>Year</b>: 2018</li>
        <li><b>Theory level</b>: introductory</li>
        <li><b>Theory type</b>: network architecture: GAN</li>
        <li><b>Used data</b>: no</li>
        <li><b>Source</b>: IEEE Signal Processing Magazine</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/8253590/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>An introductory paper that discusses several kinds of deep networks used in data-driven simulating for the image inversion. It has discussed the conventional CNN, the residual block and the auto-encoder. Beside these, note that the GAN has been an important part. It has discussed how GAN improves the performance in detail.</p>
        <p><img src="Lucas2018.png"/></p>
    </div>
  </div>
</div>

<h3 id="estimate-articulatory-mri-series-from-acoustic-signal-using-deep-architecture">Estimate articulatory MRI series from acoustic signal using deep architecture</h3>

<div class='box' style="margin-left:2em">
  <div class='row' style="margin-left:0em">
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <ul>
        <li><b>Title</b>: Estimate articulatory MRI series from acoustic signal using deep architecture</li>
        <li><b>Author</b>: Hao Li, Jianhua Tao, Minghao Yang and Bin Liu</li>
        <li><b>Year</b>: 2015</li>
        <li><b>Theory level</b>: application</li>
        <li><b>Theory type</b>: network architecture: RBM or DBN</li>
        <li><b>Used data</b>: speech audio signal</li>
        <li><b>Predicted data</b>: magnetic resonance imaging (MRI)</li>
        <li><b>Source</b>: IEEE International Conference on Acoustics, Speech and Signal Processing</li>
        </ul>
        <p style="text-indent:2em"><a href="https://ieeexplore.ieee.org/document/685708/" class="button icon fa-file-pdf-o" style="text-indent:0em">Reference</a></p>
        <ul>
        <li><b>External Reference</b>: check this link to learn implementation about restricted Boltzmann machine (RBM).</li>
        </ul>
        <p style="text-indent:2em"><a href="http://lyy1994.github.io/machine-learning/2017/04/17/RBM-tensorflow-implementation.html" class="button icon fa-file-text-o" style="text-indent:0em">Reference</a></p>
    </div>
    <div class="6u 12u(mobilep)" style="margin-bottom:2em">
        <h4 class="cancel_link">Learning abstract:</h4>
        <p>The author assumes the relationship exists between the MRI images and the speaking signal. To predict the images, the author trains a deep belief network (DBN, also viewed as stacked restricted Boltzmann machine (RBM)) to make the regression. The network architecture and the results are shown below:</p>
        <p><img src="Li2015.svg"/></p>
        <p>(I) is the architecture. Here are n stacked RBMs and a linear regression layer ($h_{n+1}$). Then the linear layer would be mapped into another RBM which is also the output layer. Note that each two layers are fully connected, but the network is trained by each layer individually first and gets fine-tunned finally.</p>
        <p>(II) is the prediction compared to the ground truth. We could tell difference for different speaking tones from the results.</p>
        <p>(III) is estimation of using different number of hidden units. It shows that the performance would not be improved apparently with increasing the network size.</p>
    </div>
  </div>
</div>

                        </div>
                    </section>
            <!-- Disqus Inject -->
                
                  <section>
    <div class="inner" id="disqus_thread"></div>
    <script type="text/javascript">

    (function() {
          
          
          if (window.location.hostname == "localhost")
                return;

          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          var disqus_shortname = 'rosenkreutz-studio';
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <div class="inner"><a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></div>
</section>
                
            </div>
            
        <!-- Footer -->
            
                <!-- Footer -->
    <footer id="footer">
        <div class="inner">
            <ul class="icons">
                
                    <li><a href="mailto:cainmagi@gmail.com" class="icon alt fa-envelope" target="_blank"><span class="label">Email</span></a></li>
                
                    <li><a href="https://weibo.com/u/5885093621" class="icon alt fa-weibo" target="_blank"><span class="label">Weibo</span></a></li>
                
                    <li><a href="https://github.com/cainmagi" class="icon alt fa-github" target="_blank"><span class="label">GitHub</span></a></li>
                
                    <li><a href="https://steamcommunity.com/id/cainmagi" class="icon alt fa-steam" target="_blank"><span class="label">Steam</span></a></li>
                
                    <li><a href="https://www.youtube.com/channel/UCzqpNK5qFMy5_cI1i0Z1nQw" class="icon alt fa-youtube-play" target="_blank"><span class="label">Youtube</span></a></li>
                
                    <li><a href="https://music.163.com/#/user/home?id=276304206" class="icon alt fa-music" target="_blank"><span class="label">Netease Music</span></a></li>
                
            </ul>
            <ul class="copyright">
                <li>&copy; Well-logging laboratory, Department of Electrical and Computer Engineering, University of Houston</li>
                
            </ul>
        </div>
    </footer>

            
        </div>

    <!-- Scripts -->
        <!-- Scripts -->
    <!-- jQuery -->
    <script src="https://cainmagi.github.io/js/jquery.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.scrolly.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.scrollex.min.js"></script>
    <script src="https://cainmagi.github.io/js/jquery.elevatezoom.js" type="text/javascript"></script>
    <script src="https://cainmagi.github.io/js/jquery.images.js"></script>
    <script src="https://cainmagi.github.io/js/skel.min.js"></script>
    <script src="https://cainmagi.github.io/js/util.js"></script>
    <script type="text/javascript" src="https://cainmagi.github.io/js/tooltipster.bundle.min.js"></script>

    

    <!-- Main JS -->
    <script src="https://cainmagi.github.io/js/main.js"></script>
    <script src="https://cainmagi.github.io/js/extensions.js"></script>
    
    
    <script src="https://cainmagi.github.io/js/title.js"></script>
    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-119875813-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


    
    
    
    <script src="https://cainmagi.github.io/js/highlight.pack.js"></script>
    <link rel="stylesheet" href="https://cainmagi.github.io/css/vs2015adp.css">
    <script>hljs.initHighlightingOnLoad();</script>
    
    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
          extensions: ["AMSmath.js", "AMSsymbols.js", "boldsymbol.js", "color.js"]
      }
    }
  });

  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
    
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

    </body>
</html>
