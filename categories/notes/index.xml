<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Rosenkreutz Studio</title>
    <link>https://cainmagi.github.io/categories/notes/</link>
    <description>Recent content in Notes on Rosenkreutz Studio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 31 Aug 2019 12:06:21 -0500</lastBuildDate>
    
	<atom:link href="https://cainmagi.github.io/categories/notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Note on Aug. 31, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190831/</link>
      <pubDate>Sat, 31 Aug 2019 12:06:21 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190831/</guid>
      <description>Introduction In the notes on May 16, we have already talked about the Adam algorithm. Combining the first-order momentum and the adaptive learning rate, Adam performs the normalization on the gradients. When the gradient is too strong, Adam correct it by reducing the learning rate, vice versa. In the aforementioned notes, we have explained why Adam is designed like this and pointed out that the proof of Adam is not correct.</description>
    </item>
    
    <item>
      <title>Chinese translated lecture notes for MATH 6366 @ UH</title>
      <link>https://cainmagi.github.io/notes/note20190820special/</link>
      <pubDate>Tue, 20 Aug 2019 02:02:02 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190820special/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Note on May 16, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190516/</link>
      <pubDate>Thu, 16 May 2019 15:03:10 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190516/</guid>
      <description>Introduction First, let us introduce Adam which is an important first-order gradient descent algorithm for updating the weights in neural network. A brief introduction in Chinese could be referred here.
The whole name of ADAM is &amp;ldquo;adaptive moment estimation&amp;rdquo;, which means it is a combination of adaptive learning rate and momentum estimation. In this article, we would discuss the theory and proof in the following paper:
Reference
In this article, we would discuss some proofs about the features of this algorithm.</description>
    </item>
    
    <item>
      <title>Special Notes on May 15, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190515special/</link>
      <pubDate>Wed, 15 May 2019 17:40:04 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190515special/</guid>
      <description>In this article, we would discuss the trick about training and testing phases for dictionary learning (sparse coding). The original work could be referred here. As extra reading materials, we suggest reading Convex Optimization for understanding how to apply Lagrangian method and Matrix Cookbook to refer some conclusions about how to calculate gradients for matrices.
The pdf version of this article could be found here:
PDF version
Solve the Lasso problem Consider the testing phase of sparse coding which could be formulated as</description>
    </item>
    
    <item>
      <title>Notes on May 05, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190505/</link>
      <pubDate>Sun, 05 May 2019 12:06:02 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190505/</guid>
      <description>Introduction In this article, we would introduce two SEG-2018 expanded abstracts. The first one is about a new learning algorithm for transient electromagnetic method (TEM) inversion, and the second one is about using deep learning to solve FWI. In the following parts, we would discuss about the problems of the two papers and analyze which parts are not well proved. The two papers are listed below.
[1]: Application of supervised descent method for transient EM data inversion:</description>
    </item>
    
    <item>
      <title>Special Notes on Mar. 4, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190304special/</link>
      <pubDate>Mon, 04 Mar 2019 02:18:28 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190304special/</guid>
      <description> Introduction In this topic, we would learn such a paper:
Solving ill-posed inverse problems using iterative deep neural networks:
Reference
 
This article is still being produced. Please wait for several days to see the full edition.
 </description>
    </item>
    
    <item>
      <title>Special Notes on Feb. 28, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190228special/</link>
      <pubDate>Thu, 28 Feb 2019 02:04:28 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190228special/</guid>
      <description>Introduction In this article, we would discuss the non-negative constrained least length problem. Due to the limitation of time, we only discuss the simplified form of this problem. The solution of this problem is derived from applying non-negative least squares algorithm.
KT conditions Check here to see Karush–Kuhn–Tucker conditions in Wikipedia.
 To solve this problem, first we need to introduce the Kuhn-Tucker (KT) theorem (or KT conditons).</description>
    </item>
    
    <item>
      <title>Special Notes on Feb. 27, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190227special/</link>
      <pubDate>Wed, 27 Feb 2019 14:47:48 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190227special/</guid>
      <description>This article is incomplete now. Due to the problem of comprehension, I decide not to continue to review this article. Maybe I would try to review it again in the future.
 Introduction In this topic, we are learning an article for studying the theory of the convergence for the neural network applied on inverse problem. To be specific, the related articles include
[1]: NETT: Solving Inverse Problems with Deep Neural Networks:</description>
    </item>
    
    <item>
      <title>Notes on Feb. 23, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190223/</link>
      <pubDate>Sat, 23 Feb 2019 07:49:01 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190223/</guid>
      <description> Notes During this week, here is no note. A detail version about the works in this week has been updated in
Reference
Slides View the slides here:
 Ooops! Your browser does not support viewing pdfs.
Download PDF
  Ooops! Your browser does not support viewing pdfs.
Download PDF
 </description>
    </item>
    
    <item>
      <title>Stochastic optimization I: from Monte-Carlo methods to Gibbs sampling</title>
      <link>https://cainmagi.github.io/notes/note20190215special/</link>
      <pubDate>Fri, 15 Feb 2019 13:04:50 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190215special/</guid>
      <description>Introduction This is a series of inspection on stochastic methods. Traditionally, an optimization problem could be solved by gradient descent methods, greedy algorithm and stochastic methods. For example, Levenberg–Marquardt algorithm is a gradient descent based methods. Another example is OMP which is used to find a local minimum of L0 penalized problem. Since the L0 norm is totally indifferentiable, we could not calculate its gradient. Therefore, such kind of problem is generally more difficult than those differentiable problems.</description>
    </item>
    
    <item>
      <title>Notes from Jan. 29, 2019 to Feb. 15, 2019</title>
      <link>https://cainmagi.github.io/notes/note20190129/</link>
      <pubDate>Tue, 29 Jan 2019 17:23:42 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20190129/</guid>
      <description>Introduction In this article we would summarize some popular solutions for the inverse problem. To be specific, here we only discuss the methods of inverse problems. Although when introducing some algorithms, we may need to explain what problem they work on, the various topics about which problem we solve is not what we concentrate on in this article.
Generally the solutions could be divided into 2 parts: optimizing methods and stochastic approaches.</description>
    </item>
    
    <item>
      <title>Special Notes on Nov. 19, 2018</title>
      <link>https://cainmagi.github.io/notes/note20181129special/</link>
      <pubDate>Mon, 19 Nov 2018 17:25:43 -0600</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20181129special/</guid>
      <description>Theory of Lovasz extension The Lovasz extension is proposed to form an interpolating function for a sub-modular set function. In this article, we are discussing about the theory and the results from this paper: &amp;ldquo;The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks&amp;rdquo;,
Reference
To get more details, we have the further reading material about Lovasz-hinge optimization: &amp;ldquo;The Lovász Hinge: A Novel Convex Surrogate for Submodular Losses&amp;ldquo;</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 24, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180824special/</link>
      <pubDate>Fri, 24 Aug 2018 03:45:41 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180824special/</guid>
      <description>Introduction Check here to see Levenberg–Marquardt algorithm in Wikipedia.
 Check this link and we could review the theory of Levenberg–Marquardt algorithm (LMA), which is used as an improvement compared to the plain first-order gradient descent method. In this short discussion, we would like to talk about how and why we could derive such a method. Then we would know that when we could use this method and when we could not.</description>
    </item>
    
    <item>
      <title>Special Notes on Aug. 13, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180813special/</link>
      <pubDate>Mon, 13 Aug 2018 01:45:25 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180813special/</guid>
      <description>Introduction In this note, we would like to discuss about an interesting idea: how to implement the conventional optimization methods in deep-learning architecture? Actually we have introduced this idea in note20180720. Here let us give a brief about this idea.
A brief about inversion First, we need to learn how the traditional inversion works. This process could be generally described as such a process:
   The workflow of a traditional inversion         Suppose we have a known data $\mathbf{y}_0$, a forward model function $\mathcal{F}$ could transform a model $\mathbf{x}$ into data $\mathbf{y}$.</description>
    </item>
    
    <item>
      <title>Notes on Jul. 20, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180720/</link>
      <pubDate>Fri, 20 Jul 2018 05:37:52 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180720/</guid>
      <description>Introduction Here we would like to discuss about the some papers using deep learning methods to enhance the performance of the traditional inverse problems. The application should be limited in analyzing the electromagnetic wave or acoustic signal. Some works just make the application of existing methods, some works propose fundamental theory and some works give us an inspiration of available architectures. According to the relevance, we would like to divide them into 3 groups.</description>
    </item>
    
    <item>
      <title>Special Notes on Jun. 05, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180605special/</link>
      <pubDate>Tue, 05 Jun 2018 00:49:31 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180605special/</guid>
      <description>Introduction The baseline of this work is existing asynchronous SGD algorithms including Hogwild, SVRG and SAGA. The author summarize the work as
 General composite optimization problem:
\begin{equation} \begin{aligned} \arg \min\limits_{\mathbf{x} \in \mathbb{R}^p} &amp; f(\mathbf{x}) + h(\mathbf{x}),\\[5pt] \mathrm{s.t.}~&amp;f(\mathbf{x}) := \frac{1}{n} \sum_{i=1}^n f_i(\mathbf{x}), \end{aligned} \end{equation}  where each $f_i$ is convex with L-Lipschitz gradient (i.e. L-smooth), and the averaging function is $\mu$-strongly convex. $h$ is convex but potentially non-smooth, and it could be decomposed block coordinate-wise as</description>
    </item>
    
    <item>
      <title>Notes on May 26, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180526/</link>
      <pubDate>Sat, 26 May 2018 02:10:27 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180526/</guid>
      <description>Set-invariant network Check here to see the article Deep Sets:
Reference
Theory This article discusses about how to build a neural network with order-invariant input set.
 Theorem 1: which function could be set invariant:  Such a function $f(\mathbb{X})$ need to be able to be decomposed as such a form: \begin{align} f(\mathbb{X}) = \rho(\sum\limits_{x \in \mathbb{X}} \phi(x)). \end{align} 
 Since the general form of the layer of neural network is $F(\mathbf{x},~\boldsymbol{\Theta}) = \sigma (\boldsymbol{\Theta}\mathbf{x})$, we could know that according to the above theorem, only when</description>
    </item>
    
    <item>
      <title>Notes on May 18, 2018</title>
      <link>https://cainmagi.github.io/notes/note20180518/</link>
      <pubDate>Fri, 25 May 2018 17:32:01 -0500</pubDate>
      
      <guid>https://cainmagi.github.io/notes/note20180518/</guid>
      <description> Notes During this week (May 18, 2018) I have not build this site. So here is no note.
Slides View the slides here:
 Ooops! Your browser does not support viewing pdfs.
Download PDF
 </description>
    </item>
    
  </channel>
</rss>